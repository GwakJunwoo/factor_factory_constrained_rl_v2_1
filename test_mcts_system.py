#!/usr/bin/env python3
"""
MCTS ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸

AlphaZero-style MCTS ì‹œìŠ¤í…œì˜ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹œì—°
"""

import os
import time
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

from factor_factory.data import ParquetCache, DATA_ROOT
from factor_factory.rlc import RLCConfig
from factor_factory.mcts import (
    PolicyValueNetwork,
    MCTSSearch, 
    MCTSFactorEnv,
    AlphaZeroTrainer
)
from factor_factory.pool import FactorPool


def test_neural_network():
    """ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  Policy-Value Network í…ŒìŠ¤íŠ¸")
    
    network = PolicyValueNetwork()
    
    # ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    obs = np.random.randn(23)
    policy_probs, value = network.predict(obs)
    
    print(f"  ì…ë ¥ í¬ê¸°: {obs.shape}")
    print(f"  ì •ì±… ì¶œë ¥: {policy_probs.shape}, í•©: {policy_probs.sum():.3f}")
    print(f"  ê°€ì¹˜ ì¶œë ¥: {value:.3f}")
    
    # ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    batch_obs = np.random.randn(32, 23)
    batch_policies, batch_values = network.predict_batch(batch_obs)
    
    print(f"  ë°°ì¹˜ ì…ë ¥: {batch_obs.shape}")
    print(f"  ë°°ì¹˜ ì •ì±…: {batch_policies.shape}")
    print(f"  ë°°ì¹˜ ê°€ì¹˜: {batch_values.shape}")
    
    print("âœ… ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")


def test_mcts_search():
    """MCTS íƒìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” MCTS Search í…ŒìŠ¤íŠ¸")
    
    network = PolicyValueNetwork()
    
    # ë”ë¯¸ í‰ê°€ í•¨ìˆ˜
    def dummy_eval(tokens):
        return np.random.uniform(-0.5, 1.0) - len(tokens) * 0.01
    
    mcts = MCTSSearch(
        network=network,
        num_simulations=100,  # í…ŒìŠ¤íŠ¸ìš© ì ì€ ìˆ˜
        evaluation_fn=dummy_eval
    )
    
    # íƒìƒ‰ ì‹¤í–‰
    start_time = time.time()
    action_probs, root_node = mcts.search(root_state=[], root_need=1)
    search_time = time.time() - start_time
    
    print(f"  íƒìƒ‰ ì‹œê°„: {search_time:.2f}ì´ˆ")
    print(f"  ë£¨íŠ¸ ë°©ë¬¸ íšŸìˆ˜: {root_node.visit_count}")
    print(f"  ìì‹ ë…¸ë“œ ìˆ˜: {len(root_node.children)}")
    print(f"  ìµœê³  ì•¡ì…˜: {mcts.get_best_action(root_node)}")
    print(f"  ì£¼ìš” ë³€í™”: {mcts.get_principal_variation(root_node, 3)}")
    
    print("âœ… MCTS íƒìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")


def test_mcts_environment():
    """MCTS í™˜ê²½ í…ŒìŠ¤íŠ¸"""
    print("ğŸ­ MCTS Environment í…ŒìŠ¤íŠ¸")
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2024-01-01', periods=500, freq='H')
    df = pd.DataFrame({
        'open': np.random.randn(500).cumsum() + 100,
        'high': np.random.randn(500).cumsum() + 102,
        'low': np.random.randn(500).cumsum() + 98,
        'close': np.random.randn(500).cumsum() + 100,
        'volume': np.random.randint(1000, 10000, 500)
    }, index=dates)
    
    config = RLCConfig()
    env = MCTSFactorEnv(df, config)
    
    # í”„ë¡œê·¸ë¨ í‰ê°€ í…ŒìŠ¤íŠ¸
    test_programs = [
        [1, 2, 12],           # SMA(CLOSE, 20)
        [1, 3, 4, 12],        # (CLOSE + HIGH) / 2
        [1, 2, 5, 12, 6]      # Complex program
    ]
    
    for i, program in enumerate(test_programs):
        result = env.evaluate_program(program)
        print(f"  í”„ë¡œê·¸ë¨ {i+1}: {program}")
        print(f"    ì„±ê³µ: {result['success']}")
        print(f"    ë³´ìƒ: {result['reward']:.4f}")
        if result.get('formula'):
            print(f"    ê³µì‹: {result['formula']}")
    
    stats = env.get_statistics()
    print(f"  í™˜ê²½ í†µê³„: {stats}")
    
    print("âœ… MCTS í™˜ê²½ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")


def test_alphazero_trainer():
    """AlphaZero íŠ¸ë ˆì´ë„ˆ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– AlphaZero Trainer í…ŒìŠ¤íŠ¸")
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2024-01-01', periods=200, freq='H')  # ì‘ì€ ë°ì´í„°ì…‹
    df = pd.DataFrame({
        'open': np.random.randn(200).cumsum() + 100,
        'high': np.random.randn(200).cumsum() + 102,
        'low': np.random.randn(200).cumsum() + 98,
        'close': np.random.randn(200).cumsum() + 100,
        'volume': np.random.randint(1000, 10000, 200)
    }, index=dates)
    
    config = RLCConfig()
    env = MCTSFactorEnv(df, config)
    network = PolicyValueNetwork()
    factor_pool = FactorPool("test_mcts_factor_pool")
    
    # í…ŒìŠ¤íŠ¸ìš© ê°„ì†Œí™”ëœ íŠ¸ë ˆì´ë„ˆ
    trainer = AlphaZeroTrainer(
        env=env,
        network=network,
        factor_pool=factor_pool,
        episodes_per_iteration=5,    # ì ì€ ì—í”¼ì†Œë“œ
        mcts_simulations=50,         # ì ì€ ì‹œë®¬ë ˆì´ì…˜
        training_epochs=2,           # ì ì€ ì—í¬í¬
        evaluation_episodes=3,       # ì ì€ í‰ê°€
        evaluation_interval=1,       # ë§¤ë²ˆ í‰ê°€
        save_interval=2,             # ìì£¼ ì €ì¥
        checkpoint_dir="test_mcts_checkpoints"
    )
    
    print(f"  í…ŒìŠ¤íŠ¸ìš© ì„¤ì •:")
    print(f"    ì—í”¼ì†Œë“œ/ë°˜ë³µ: {trainer.episodes_per_iteration}")
    print(f"    MCTS ì‹œë®¬ë ˆì´ì…˜: {trainer.mcts.num_simulations}")
    print(f"    í•™ìŠµ ì—í¬í¬: {trainer.training_epochs}")
    
    # ì§§ì€ í•™ìŠµ ì‹¤í–‰
    start_time = time.time()
    trainer.train(num_iterations=2)  # 2ë²ˆë§Œ ë°˜ë³µ
    training_time = time.time() - start_time
    
    print(f"  í•™ìŠµ ì‹œê°„: {training_time:.1f}ì´ˆ")
    print(f"  ë°œê²¬ëœ íŒ©í„°: {len(trainer.discovered_factors)}ê°œ")
    print(f"  ìµœê³  ì„±ëŠ¥: {trainer.best_performance:.4f}")
    
    # Factor Pool í†µê³„
    pool_stats = factor_pool.get_statistics()
    print(f"  Factor Pool: {pool_stats}")
    
    print("âœ… AlphaZero íŠ¸ë ˆì´ë„ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")


def integration_test():
    """í†µí•© í…ŒìŠ¤íŠ¸"""
    print("ğŸ”— MCTS ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸")
    
    try:
        # ì‹¤ì œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            cache = ParquetCache(DATA_ROOT)
            df = cache.get("BTCUSDT", "1h")
            df = df.tail(1000)  # ìµœê·¼ 1000ê°œ ë°ì´í„°ë§Œ ì‚¬ìš©
            print(f"  ì‹¤ì œ ë°ì´í„° ì‚¬ìš©: {df.shape}")
        except:
            # ë”ë¯¸ ë°ì´í„° ì‚¬ìš©
            dates = pd.date_range('2024-01-01', periods=1000, freq='H')
            df = pd.DataFrame({
                'open': np.random.randn(1000).cumsum() + 100,
                'high': np.random.randn(1000).cumsum() + 102, 
                'low': np.random.randn(1000).cumsum() + 98,
                'close': np.random.randn(1000).cumsum() + 100,
                'volume': np.random.randint(1000, 10000, 1000)
            }, index=dates)
            print(f"  ë”ë¯¸ ë°ì´í„° ì‚¬ìš©: {df.shape}")
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
        config = RLCConfig()
        env = MCTSFactorEnv(df, config)
        network = PolicyValueNetwork()
        
        # MCTSë¡œ í”„ë¡œê·¸ë¨ ìƒì„±
        mcts = MCTSSearch(
            network=network,
            num_simulations=200,
            evaluation_fn=lambda tokens: env.evaluate_program(tokens)['reward']
        )
        
        print("  MCTS íƒìƒ‰ìœ¼ë¡œ í”„ë¡œê·¸ë¨ ìƒì„± ì¤‘...")
        action_probs, root_node = mcts.search([], 1)
        
        # ì—¬ëŸ¬ í”„ë¡œê·¸ë¨ ìƒì„± ë° í‰ê°€
        programs_found = 0
        total_attempts = 5
        
        for attempt in range(total_attempts):
            try:
                # í”„ë¡œê·¸ë¨ ìƒì„±
                program = []
                node = root_node
                
                for _ in range(21):  # ìµœëŒ€ ê¸¸ì´
                    if not node.children or node.is_terminal:
                        break
                    
                    # ê°€ì¥ ë°©ë¬¸ ë§ì€ ìì‹ ì„ íƒ
                    best_child = node.get_best_child()
                    if best_child is None:
                        break
                    
                    program.append(best_child.action)
                    node = best_child
                
                if program:
                    result = env.evaluate_program(program)
                    if result['success']:
                        programs_found += 1
                        print(f"    í”„ë¡œê·¸ë¨ {programs_found}: ë³´ìƒ={result['reward']:.4f}")
                        if result.get('formula'):
                            print(f"      ê³µì‹: {result['formula']}")
            
            except Exception as e:
                print(f"    ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
                continue
        
        success_rate = programs_found / total_attempts
        print(f"  ì„±ê³µë¥ : {programs_found}/{total_attempts} ({success_rate:.1%})")
        
        env_stats = env.get_statistics()
        print(f"  ìµœì¢… í™˜ê²½ í†µê³„: {env_stats}")
        
        print("âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")
        
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\n")


def main():
    print("ğŸš€ MCTS ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    print("="*60)
    
    # ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
    test_neural_network()
    test_mcts_search() 
    test_mcts_environment()
    test_alphazero_trainer()
    
    # í†µí•© í…ŒìŠ¤íŠ¸
    integration_test()
    
    print("="*60)
    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    print("\nğŸ“š MCTS ì‹œìŠ¤í…œ ì‚¬ìš©ë²•:")
    print("1. í•™ìŠµ: python -m factor_factory.scripts.cli_mcts_train --symbol BTCUSDT --iterations 50")
    print("2. ì¶”ë¡ : python -m factor_factory.scripts.cli_mcts_infer --model best_model.pt --symbol BTCUSDT --num-searches 20")
    print("3. PPOì™€ ë¹„êµ: python -m factor_factory.scripts.cli_mcts_infer --compare-ppo ppo_results.json")
    
    print("\nğŸ”„ PPO vs MCTS ë¹„êµ:")
    print("- PPO: ì—°ì†ì  ì •ì±… ê°œì„ , ë¹ ë¥¸ í•™ìŠµ, ì•ˆì •ì ")
    print("- MCTS: íŠ¸ë¦¬ íƒìƒ‰ ê¸°ë°˜, ì •í™•í•œ í‰ê°€, ë” ë‚˜ì€ ì¥ê¸° ê³„íš")
    print("- ë‘ ë°©ë²•ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ìµœê³ ì˜ íŒ©í„° ë°œê²¬ ê°€ëŠ¥!")


if __name__ == "__main__":
    main()
