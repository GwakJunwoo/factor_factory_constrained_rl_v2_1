#!/usr/bin/env python3
"""
í•©ë¦¬ì ìœ¼ë¡œ ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œ
- ì´ì¤‘ í˜ë„í‹° ì œê±° (PnLì— ì´ë¯¸ ê±°ë˜ë¹„ìš© ë°˜ì˜ë¨)
- ì‹¤ìš©ì ì´ê³  ê· í˜•ì¡íŒ ë³´ìƒ êµ¬ì¡°
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

@dataclass
class RationalRLCConfig:
    """í•©ë¦¬ì ìœ¼ë¡œ ê°œì„ ëœ RL í™˜ê²½ ì„¤ì •"""
    max_len: int = 21
    
    # === í•µì‹¬ í˜ë„í‹° (ê°„ì†Œí™”) ===
    lambda_complexity: float = 0.005    # ë³µì¡ë„ í˜ë„í‹° (depth + length)
    lambda_overfitting: float = 1.0     # ê³¼ì í•© ë°©ì§€ (MDD ê¸°ë°˜)
    lambda_instability: float = 0.3     # ë¶ˆì•ˆì •ì„± í˜ë„í‹° (ë³€ë™ì„± ê¸°ë°˜)
    
    # === í’ˆì§ˆ ë³´ë„ˆìŠ¤ ===
    alpha_consistency: float = 0.2      # ì¼ê´€ì„± ë³´ë„ˆìŠ¤
    alpha_efficiency: float = 0.1       # íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤ (ìŠ¹ë¥  ê¸°ë°˜)
    
    # === ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ===
    reward_scale: float = 10.0          # ì£¼ ë³´ìƒ ìŠ¤ì¼€ì¼ë§
    penalty_cap: float = 2.0            # í˜ë„í‹° ìƒí•œ
    
    # === ì •ê·œí™” ì„¤ì • ===
    normalize_rewards: bool = True
    reward_window: int = 200

class RationalRewardSystem:
    """í•©ë¦¬ì ìœ¼ë¡œ ì„¤ê³„ëœ ë³´ìƒ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: RationalRLCConfig):
        self.config = config
        self.episode_count = 0
        self.reward_history = []
        
    def calculate_reward(
        self,
        pnl: pd.Series,
        equity: pd.Series, 
        signal: pd.Series,
        tokens: List[int],
        depth: int
    ) -> Dict:
        """ë©”ì¸ ë³´ìƒ ê³„ì‚° í•¨ìˆ˜"""
        
        self.episode_count += 1
        
        # 1. ì£¼ ë³´ìƒ: PnL (ì´ë¯¸ ê±°ë˜ë¹„ìš© ë°˜ì˜ë¨)
        net_pnl = float(pnl.sum())
        main_reward = net_pnl * self.config.reward_scale
        
        # 2. ë³µì¡ë„ í˜ë„í‹° (êµ¬ì¡°ì  ë‹¨ìˆœí•¨ ê²©ë ¤)
        complexity_penalty = self._calculate_complexity_penalty(tokens, depth)
        
        # 3. ê³¼ì í•© í˜ë„í‹° (MDD ê¸°ë°˜)
        overfitting_penalty = self._calculate_overfitting_penalty(equity)
        
        # 4. ë¶ˆì•ˆì •ì„± í˜ë„í‹° (ë³€ë™ì„± ê¸°ë°˜)
        instability_penalty = self._calculate_instability_penalty(pnl)
        
        # 5. í’ˆì§ˆ ë³´ë„ˆìŠ¤
        consistency_bonus = self._calculate_consistency_bonus(pnl)
        efficiency_bonus = self._calculate_efficiency_bonus(pnl)
        
        # êµ¬ì„± ìš”ì†Œ
        components = {
            'main_reward': main_reward,
            'complexity_penalty': -complexity_penalty,
            'overfitting_penalty': -overfitting_penalty, 
            'instability_penalty': -instability_penalty,
            'consistency_bonus': consistency_bonus,
            'efficiency_bonus': efficiency_bonus
        }
        
        # ì´ ë³´ìƒ ê³„ì‚°
        total_penalty = complexity_penalty + overfitting_penalty + instability_penalty
        total_bonus = consistency_bonus + efficiency_bonus
        
        # í˜ë„í‹° ìº¡ ì ìš© (ê³¼ë„í•œ í˜ë„í‹° ë°©ì§€)
        capped_penalty = min(total_penalty, self.config.penalty_cap)
        
        raw_reward = main_reward - capped_penalty + total_bonus
        
        # ë³´ìƒ ì •ê·œí™”
        if self.config.normalize_rewards:
            final_reward = self._normalize_reward(raw_reward)
        else:
            final_reward = raw_reward
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.reward_history.append(final_reward)
        
        return {
            'total_reward': final_reward,
            'raw_reward': raw_reward,
            'components': components,
            'episode': self.episode_count,
            'penalty_capped': total_penalty > self.config.penalty_cap
        }
    
    def _calculate_complexity_penalty(self, tokens: List[int], depth: int) -> float:
        """ë³µì¡ë„ í˜ë„í‹°: ê¹Šì´ + ê¸¸ì´"""
        
        # ê¹Šì´ í˜ë„í‹° (ì§€ìˆ˜ì  ì¦ê°€)
        depth_penalty = (depth / 10) ** 1.5
        
        # ê¸¸ì´ í˜ë„í‹° (ì„ í˜• ì¦ê°€)
        length_penalty = len(tokens) / 100
        
        complexity = depth_penalty + length_penalty
        return self.config.lambda_complexity * complexity
    
    def _calculate_overfitting_penalty(self, equity: pd.Series) -> float:
        """ê³¼ì í•© í˜ë„í‹°: MDD ê¸°ë°˜"""
        
        try:
            # ìµœëŒ€ ë‚™í­ ê³„ì‚°
            cummax = equity.cummax()
            drawdown = (equity - cummax) / cummax
            max_drawdown = abs(drawdown.min())
            
            # MDDê°€ í´ìˆ˜ë¡ ê³¼ì í•© ê°€ëŠ¥ì„± ë†’ìŒ
            # 20% ì´ìƒ MDDëŠ” ê°•í•˜ê²Œ í˜ë„í‹°
            if max_drawdown > 0.2:
                overfitting = (max_drawdown - 0.2) * 2.0 + 0.2
            else:
                overfitting = max_drawdown
                
            return self.config.lambda_overfitting * overfitting
            
        except:
            return 0.0
    
    def _calculate_instability_penalty(self, pnl: pd.Series) -> float:
        """ë¶ˆì•ˆì •ì„± í˜ë„í‹°: ë³€ë™ì„± ê¸°ë°˜"""
        
        try:
            # ì¼ì¼ PnL ë³€ë™ì„±
            pnl_volatility = pnl.std()
            
            # ë³€ë™ì„±ì´ í‰ê·  ìˆ˜ìµë¥  ëŒ€ë¹„ ë„ˆë¬´ í´ ë•Œ í˜ë„í‹°
            mean_pnl = abs(pnl.mean())
            if mean_pnl > 1e-8:
                volatility_ratio = pnl_volatility / mean_pnl
                # ë³€ë™ì„±/ìˆ˜ìµë¥  ë¹„ìœ¨ì´ 5ë°° ì´ìƒì´ë©´ ë¶ˆì•ˆì •
                instability = max(0, volatility_ratio - 5.0) / 10.0
            else:
                # ìˆ˜ìµë¥ ì´ 0ì— ê°€ê¹Œìš°ë©´ ë³€ë™ì„± ìì²´ê°€ ë¬¸ì œ
                instability = pnl_volatility * 100
                
            return self.config.lambda_instability * instability
            
        except:
            return 0.0
    
    def _calculate_consistency_bonus(self, pnl: pd.Series) -> float:
        """ì¼ê´€ì„± ë³´ë„ˆìŠ¤: ì•ˆì •ì  ìˆ˜ìµ íŒ¨í„´ ê²©ë ¤"""
        
        try:
            if len(pnl) < 10:
                return 0.0
                
            # ìˆ˜ìµë¥ ì˜ ë¶€í˜¸ ì¼ê´€ì„±
            positive_periods = (pnl > 0).sum()
            total_periods = len(pnl)
            
            # ìŠ¹ë¥ ì´ 50%ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì¼ê´€ì„± ìˆìŒ
            win_rate = positive_periods / total_periods
            consistency_score = abs(win_rate - 0.5)  # 0~0.5
            
            # ì¶”ê°€: ì—°ì† ì†ì‹¤ ê¸°ê°„ í™•ì¸
            consecutive_losses = self._max_consecutive_losses(pnl)
            if consecutive_losses > 10:  # 10ì¼ ì—°ì† ì†ì‹¤ì€ í˜ë„í‹°
                consistency_score *= 0.5
                
            return self.config.alpha_consistency * consistency_score
            
        except:
            return 0.0
    
    def _calculate_efficiency_bonus(self, pnl: pd.Series) -> float:
        """íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤: ë†’ì€ ìŠ¹ë¥  + ìˆ˜ìµ/ì†ì‹¤ ë¹„ìœ¨"""
        
        try:
            if len(pnl) < 5:
                return 0.0
                
            positive_pnl = pnl[pnl > 0]
            negative_pnl = pnl[pnl < 0]
            
            if len(positive_pnl) == 0 or len(negative_pnl) == 0:
                return 0.0
                
            # í‰ê·  ìˆ˜ìµ ëŒ€ í‰ê·  ì†ì‹¤ ë¹„ìœ¨
            avg_profit = positive_pnl.mean()
            avg_loss = abs(negative_pnl.mean())
            
            profit_loss_ratio = avg_profit / avg_loss if avg_loss > 0 else 0
            
            # ìŠ¹ë¥ 
            win_rate = len(positive_pnl) / len(pnl)
            
            # íš¨ìœ¨ì„± = ìŠ¹ë¥  * ìˆ˜ìµì†ì‹¤ë¹„ìœ¨
            efficiency = win_rate * min(profit_loss_ratio, 3.0)  # ë¹„ìœ¨ ìº¡
            
            return self.config.alpha_efficiency * efficiency
            
        except:
            return 0.0
    
    def _max_consecutive_losses(self, pnl: pd.Series) -> int:
        """ìµœëŒ€ ì—°ì† ì†ì‹¤ ê¸°ê°„ ê³„ì‚°"""
        
        consecutive = 0
        max_consecutive = 0
        
        for p in pnl:
            if p <= 0:
                consecutive += 1
                max_consecutive = max(max_consecutive, consecutive)
            else:
                consecutive = 0
                
        return max_consecutive
    
    def _normalize_reward(self, raw_reward: float) -> float:
        """ë³´ìƒ ì •ê·œí™”"""
        
        self.reward_history.append(raw_reward)
        
        # ìœˆë„ìš° í¬ê¸° ìœ ì§€
        if len(self.reward_history) > self.config.reward_window:
            self.reward_history.pop(0)
        
        # ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ Z-score ì •ê·œí™”
        if len(self.reward_history) > 20:
            history = np.array(self.reward_history[-self.config.reward_window:])
            mean = history.mean()
            std = history.std() + 1e-8
            
            normalized = (raw_reward - mean) / std
            
            # í´ë¦¬í•‘ ë° ìŠ¤ì¼€ì¼ë§
            clipped = np.clip(normalized, -3.0, 3.0)
            final = np.tanh(clipped)  # [-1, 1] ë²”ìœ„
            
            return final
        else:
            # ì´ˆê¸°ì—ëŠ” ë‹¨ìˆœ ìŠ¤ì¼€ì¼ë§
            return np.tanh(raw_reward)

# ê°„ì†Œí™”ëœ í™˜ê²½ ì„¤ì • í´ë˜ìŠ¤
@dataclass 
class SimplifiedRLCConfig:
    """ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°„ì†Œí™”ëœ ì„¤ì •"""
    max_len: int = 21
    
    # === ë‹¨ìˆœí™”ëœ í˜ë„í‹° (ê¸°ì¡´ ëŒ€ë¹„) ===
    lambda_depth: float = 0.008       # ê¸°ì¡´ 0.002 â†’ 0.008 (4ë°° ì¦ê°€)
    lambda_const1: float = 0.3        # ê¸°ì¡´ 2.0 â†’ 0.3 (ìƒìˆ˜ ì‚¬ìš© í—ˆìš©)  
    lambda_std: float = 0.05          # ê¸°ì¡´ 0.5 â†’ 0.05 (ë³€ë™ì„± í˜ë„í‹° ëŒ€í­ ì™„í™”)
    
    # === ì œê±°ëœ í˜ë„í‹° ===
    # lambda_turnover: ì œê±°ë¨ (PnLì— ì´ë¯¸ ê±°ë˜ë¹„ìš© ë°˜ì˜)
    
    # === ìƒˆë¡œìš´ ì•ˆì •ì„± í˜ë„í‹° ===
    lambda_drawdown: float = 1.5      # MDD í˜ë„í‹° ì¶”ê°€
    
    # ê¸°íƒ€ ì„¤ì •
    commission: float = 0.0008
    slippage: float = 0.0015
    leverage: int = 1
    long_threshold: float = 1.5
    short_threshold: float = -1.5

def demonstrate_rational_rewards():
    """í•©ë¦¬ì  ë³´ìƒ ì‹œìŠ¤í…œ ì‹œì—°"""
    
    print("ğŸ¯ í•©ë¦¬ì  ë³´ìƒ ì‹œìŠ¤í…œ ì‹œì—°")
    print("=" * 50)
    
    # ì„¤ì •
    config = RationalRLCConfig()
    reward_system = RationalRewardSystem(config)
    
    # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=1000, freq='H')
    
    scenarios = [
        # (ì´ë¦„, PnL íŠ¹ì„±, í† í° íŠ¹ì„±)
        ("ê· í˜•ì¡íŒ ì „ëµ", "stable_profit", [1,2,3,4,5], 3),
        ("ê³ ìˆ˜ìµ ê³ ìœ„í—˜", "high_vol", [1,2,3,4,5,6,7,8,9], 5), 
        ("ê³¼ë„í•˜ê²Œ ë³µì¡", "normal", [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], 8),
        ("ìƒìˆ˜ ì˜ì¡´", "stable_profit", [12,12,12,12], 1),
        ("ë¶ˆì•ˆì •í•œ ì „ëµ", "unstable", [1,2,3,4,5], 3)
    ]
    
    print(f"{'ì „ëµ':15s} {'ì´ë³´ìƒ':>8s} {'ì£¼ë³´ìƒ':>8s} {'ë³µì¡ë„':>8s} {'ê³¼ì í•©':>8s} {'ë¶ˆì•ˆì •':>8s}")
    print("-" * 70)
    
    for name, pnl_type, tokens, depth in scenarios:
        # PnL ìƒì„±
        if pnl_type == "stable_profit":
            pnl = pd.Series(np.random.normal(0.0005, 0.002, 1000), index=dates)
        elif pnl_type == "high_vol": 
            pnl = pd.Series(np.random.normal(0.001, 0.01, 1000), index=dates)
        elif pnl_type == "unstable":
            pnl = pd.Series(np.random.normal(0, 0.005, 1000), index=dates)
            pnl.iloc[100:200] = -0.02  # í° ì†ì‹¤ êµ¬ê°„
        else:  # normal
            pnl = pd.Series(np.random.normal(0.0003, 0.003, 1000), index=dates)
        
        equity = (1 + pnl).cumprod()
        signal = pd.Series(np.random.normal(0, 1, 1000), index=dates)
        
        # ë³´ìƒ ê³„ì‚°
        result = reward_system.calculate_reward(pnl, equity, signal, tokens, depth)
        
        comp = result['components']
        print(f"{name:15s} {result['total_reward']:8.4f} {comp['main_reward']:8.4f} "
              f"{comp['complexity_penalty']:8.4f} {comp['overfitting_penalty']:8.4f} {comp['instability_penalty']:8.4f}")
        
        # ì²« ë²ˆì§¸ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ìƒì„¸ ì¶œë ¥
        if name == "ê· í˜•ì¡íŒ ì „ëµ":
            print(f"\nğŸ“Š ìƒì„¸ ë¶„ì„: {name}")
            print(f"   - ì¼ê´€ì„± ë³´ë„ˆìŠ¤: {comp['consistency_bonus']:8.4f}")
            print(f"   - íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤: {comp['efficiency_bonus']:8.4f}")
            print(f"   - í˜ë„í‹° ìº¡ ì ìš©: {'Yes' if result['penalty_capped'] else 'No'}")
            print(f"   - Raw ë³´ìƒ: {result['raw_reward']:8.4f}")
            print()

def compare_old_vs_new():
    """ê¸°ì¡´ vs ìƒˆë¡œìš´ ë³´ìƒ ì‹œìŠ¤í…œ ë¹„êµ"""
    
    print("\nğŸ”„ ê¸°ì¡´ vs ìƒˆë¡œìš´ ë³´ìƒ ì‹œìŠ¤í…œ ë¹„êµ")
    print("=" * 50)
    
    print("ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ë¬¸ì œì :")
    print("âŒ ê±°ë˜íšŸìˆ˜ ì´ì¤‘ í˜ë„í‹° (PnLì— ì´ë¯¸ ë°˜ì˜ë¨)")
    print("âŒ ê³¼ë„í•œ ìƒìˆ˜ ì‚¬ìš© ì–µì œ (lambda_const1=2.0)")  
    print("âŒ ë¶€ì ì ˆí•œ ë³€ë™ì„± í˜ë„í‹° (ë‚®ì€ ë³€ë™ì„±ì— í° í˜ë„í‹°)")
    print("âŒ ë³µì¡ë„ í˜ë„í‹° ë¶€ì¡± (lambda_depth=0.002)")
    
    print("\nìƒˆë¡œìš´ ì‹œìŠ¤í…œì˜ ê°œì„ ì :")
    print("âœ… ê±°ë˜íšŸìˆ˜ í˜ë„í‹° ì œê±° (ì´ì¤‘ í˜ë„í‹° í•´ê²°)")
    print("âœ… ìƒìˆ˜ ì‚¬ìš© í—ˆìš© (lambda_const1=0.3)")
    print("âœ… í•©ë¦¬ì  ë³€ë™ì„± ì²˜ë¦¬ (ì•ˆì •ì„± vs ë¶ˆì•ˆì •ì„± êµ¬ë¶„)")
    print("âœ… ê°•í™”ëœ ë³µì¡ë„ ì–µì œ (lambda_complexity=0.005)")
    print("âœ… MDD ê¸°ë°˜ ê³¼ì í•© ë°©ì§€")
    print("âœ… ì¼ê´€ì„±/íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤ ì¶”ê°€")
    print("âœ… í˜ë„í‹° ìº¡ìœ¼ë¡œ ê³¼ë„í•œ ì–µì œ ë°©ì§€")
    
    print("\nê¶Œì¥ ì ìš© ìˆœì„œ:")
    print("1ï¸âƒ£ ì¦‰ì‹œ ì ìš©: lambda_turnover ì œê±°, ê°€ì¤‘ì¹˜ ì¡°ì •")
    print("2ï¸âƒ£ ë‹¨ê¸° ì ìš©: MDD í˜ë„í‹° ì¶”ê°€")  
    print("3ï¸âƒ£ ì¤‘ê¸° ì ìš©: í’ˆì§ˆ ë³´ë„ˆìŠ¤ ì‹œìŠ¤í…œ")
    print("4ï¸âƒ£ ì¥ê¸° ì ìš©: ì™„ì „í•œ í•©ë¦¬ì  ë³´ìƒ ì‹œìŠ¤í…œ")

if __name__ == "__main__":
    demonstrate_rational_rewards()
    compare_old_vs_new()
