#!/usr/bin/env python3
"""
ê°•í™”í•™ìŠµ ë³´ìƒ ì‹œìŠ¤í…œ ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    plt.rcParams['font.family'] = 'Malgun Gothic'
except:
    plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def analyze_reward_components():
    """ë³´ìƒ ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ë¶„ì„"""
    
    print("ğŸ† FACTOR FACTORY ê°•í™”í•™ìŠµ ë³´ìƒ ì‹œìŠ¤í…œ ë¶„ì„")
    print("=" * 60)
    
    # ì‹¤ì œ ì„¤ì •ê°’
    config = {
        'lambda_depth': 0.002,
        'lambda_turnover': 0.0005, 
        'lambda_const1': 2.0,
        'lambda_std': 0.5,
        'length_penalty': 0.0005,
        'commission': 0.0008,
        'slippage': 0.0015,
        'signal_delay': 1,
        'execution_delay': 1
    }
    
    print("ğŸ“Š í˜„ì¬ ì„¤ì •ê°’:")
    for key, value in config.items():
        print(f"  {key:20}: {value}")
    
    # ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
    scenarios = [
        {"name": "Simple Strategy", "pnl": 0.05, "depth": 2, "trades": 20, "const_ratio": 0.1, "std": 2.0},
        {"name": "Complex Strategy", "pnl": 0.08, "depth": 5, "trades": 100, "const_ratio": 0.0, "std": 3.0},
        {"name": "Overfit Strategy", "pnl": 0.12, "depth": 8, "trades": 500, "const_ratio": 0.0, "std": 1.0},
        {"name": "Constant Strategy", "pnl": 0.02, "depth": 1, "trades": 5, "const_ratio": 0.8, "std": 0.5},
    ]
    
    print(f"\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë³´ìƒ ë¶„ì„:")
    print("-" * 80)
    print(f"{'Strategy':<20} {'PnL':>8} {'Final':>8} {'Depth':>6} {'Trade':>6} {'Const':>6} {'Std':>6}")
    print("-" * 80)
    
    for scenario in scenarios:
        pnl = scenario["pnl"]
        depth_pen = config['lambda_depth'] * scenario["depth"]
        turnover_pen = config['lambda_turnover'] * scenario["trades"] 
        const_pen = config['lambda_const1'] * scenario["const_ratio"]
        std_pen = config['lambda_std'] / scenario["std"]
        
        final_reward = pnl - depth_pen - turnover_pen - const_pen - std_pen
        
        print(f"{scenario['name']:<20} {pnl:>7.4f} {final_reward:>7.4f} "
              f"{-depth_pen:>6.4f} {-turnover_pen:>6.4f} {-const_pen:>6.4f} {-std_pen:>6.4f}")
    
    return scenarios, config

def plot_reward_sensitivity():
    """ë³´ìƒ í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('ë³´ìƒ í•¨ìˆ˜ íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„', fontsize=16, fontweight='bold')
    
    # ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤
    base_pnl = 0.05
    base_depth = 3
    base_trades = 50
    base_const = 0.2
    base_std = 2.0
    
    # 1. ê¹Šì´ í˜ë„í‹° ì˜í–¥
    ax1 = axes[0, 0]
    depths = np.arange(1, 10)
    lambda_depths = [0.001, 0.002, 0.005, 0.01]
    
    for lambda_d in lambda_depths:
        rewards = [base_pnl - lambda_d * d for d in depths]
        ax1.plot(depths, rewards, label=f'Î»_depth={lambda_d}', marker='o')
    
    ax1.set_xlabel('Tree Depth')
    ax1.set_ylabel('Reward')
    ax1.set_title('ê¹Šì´ í˜ë„í‹° ì˜í–¥')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ê±°ë˜íšŸìˆ˜ í˜ë„í‹° ì˜í–¥  
    ax2 = axes[0, 1]
    trades = np.arange(10, 200, 10)
    lambda_turnovers = [0.0001, 0.0005, 0.001, 0.002]
    
    for lambda_t in lambda_turnovers:
        rewards = [base_pnl - lambda_t * t for t in trades]
        ax2.plot(trades, rewards, label=f'Î»_turnover={lambda_t}', marker='s')
    
    ax2.set_xlabel('Number of Trades')
    ax2.set_ylabel('Reward') 
    ax2.set_title('ê±°ë˜íšŸìˆ˜ í˜ë„í‹° ì˜í–¥')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ìƒìˆ˜ ì‚¬ìš© í˜ë„í‹°
    ax3 = axes[1, 0]
    const_ratios = np.linspace(0, 1, 20)
    lambda_consts = [0.5, 1.0, 2.0, 5.0]
    
    for lambda_c in lambda_consts:
        rewards = [base_pnl - lambda_c * c for c in const_ratios]
        ax3.plot(const_ratios, rewards, label=f'Î»_const={lambda_c}', marker='^')
    
    ax3.set_xlabel('Constant Usage Ratio')
    ax3.set_ylabel('Reward')
    ax3.set_title('ìƒìˆ˜ ì‚¬ìš© í˜ë„í‹° ì˜í–¥')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ë³€ë™ì„± í˜ë„í‹°
    ax4 = axes[1, 1]
    stds = np.linspace(0.5, 5.0, 20)
    lambda_stds = [0.1, 0.5, 1.0, 2.0]
    
    for lambda_s in lambda_stds:
        rewards = [base_pnl - lambda_s / s for s in stds]
        ax4.plot(stds, rewards, label=f'Î»_std={lambda_s}', marker='d')
    
    ax4.set_xlabel('Signal Standard Deviation')
    ax4.set_ylabel('Reward')
    ax4.set_title('ë³€ë™ì„± í˜ë„í‹° ì˜í–¥')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ì €ì¥
    output_dir = Path("reward_analysis")
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / "reward_sensitivity.png", dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ë¯¼ê°ë„ ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {output_dir / 'reward_sensitivity.png'}")
    
    return fig

def plot_realistic_trading_timeline():
    """í˜„ì‹¤ì  ê±°ë˜ íƒ€ì´ë° ì‹œê°í™”"""
    
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # ì‹œê°„ì¶• ìƒì„±
    times = np.arange(0, 10)
    price_data = 100 + np.cumsum(np.random.randn(10) * 0.5)
    
    # íƒ€ì„ë¼ì¸ ì‹œê°í™”
    ax.plot(times, price_data, 'k-', linewidth=2, label='Price', alpha=0.7)
    
    # ì‹ í˜¸ ìƒì„± ì‹œì  (t=3)
    signal_time = 3
    ax.axvline(signal_time, color='blue', linestyle='--', alpha=0.7, label='Signal Generated')
    ax.text(signal_time + 0.1, price_data[signal_time] + 1, 'Signal\nGenerated\n(uses data tâ‰¤3)', 
            fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
    
    # ê±°ë˜ ê²°ì • ì‹œì  (t=4, signal_delay=1)
    decision_time = signal_time + 1
    ax.axvline(decision_time, color='orange', linestyle='--', alpha=0.7, label='Trading Decision')
    ax.text(decision_time + 0.1, price_data[decision_time] - 1, 'Trading\nDecision\n(+1 delay)', 
            fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
    
    # ì‹¤ì œ ì²´ê²° ì‹œì  (t=5, execution_delay=1)
    execution_time = decision_time + 1
    ax.axvline(execution_time, color='red', linestyle='--', alpha=0.7, label='Order Execution')
    ax.text(execution_time + 0.1, price_data[execution_time] + 1, 'Order\nExecution\n(+1 delay)', 
            fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"))
    
    # ìˆ˜ìµë¥  ì‹¤í˜„ ì‹œì  (t=6)
    pnl_time = execution_time + 1
    ax.axvline(pnl_time, color='green', linestyle='--', alpha=0.7, label='PnL Realization')
    ax.text(pnl_time + 0.1, price_data[pnl_time] - 1, 'PnL\nRealization\n(position Ã— return)', 
            fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
    
    # ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ë°©ì§€ ì˜ì—­ í‘œì‹œ
    ax.axvspan(signal_time + 0.01, 10, alpha=0.1, color='red', 
               label='Future Data\n(NOT USED)')
    
    ax.set_xlabel('Time Periods', fontsize=12)
    ax.set_ylabel('Price', fontsize=12)
    ax.set_title('í˜„ì‹¤ì  ê±°ë˜ íƒ€ì´ë° ë° ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ë°©ì§€', fontsize=14, fontweight='bold')
    ax.legend(loc='upper left')
    ax.grid(True, alpha=0.3)
    
    # ì •ë³´ ìƒì ì¶”ê°€
    info_text = """
í˜„ì‹¤ì  ê±°ë˜ ì¡°ê±´:
â€¢ Signal Delay: 1 period
â€¢ Execution Delay: 1 period  
â€¢ Commission: 0.08%
â€¢ Slippage: 0.15%
â€¢ Market Impact: 0.02%
â€¢ Total Delay: 3 periods
    """
    ax.text(0.02, 0.98, info_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="whitesmoke"))
    
    plt.tight_layout()
    
    # ì €ì¥
    output_dir = Path("reward_analysis")
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / "realistic_trading_timeline.png", dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ê±°ë˜ íƒ€ì´ë° ì°¨íŠ¸ ì €ì¥: {output_dir / 'realistic_trading_timeline.png'}")
    
    return fig

def generate_learning_progress_simulation():
    """í•™ìŠµ ì§„í–‰ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜"""
    
    # ê°€ìƒì˜ í•™ìŠµ ë°ì´í„° ìƒì„±
    episodes = np.arange(1, 1001)
    
    # ì´ˆê¸°ì—ëŠ” ë‚®ê³  ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë³´ìƒ
    base_reward = -0.5 + 0.8 * (1 - np.exp(-episodes / 200))
    noise = np.random.normal(0, 0.2, len(episodes))
    rewards = base_reward + noise
    
    # ì´ë™í‰ê· ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ íŠ¸ë Œë“œ ìƒì„±
    window = 50
    reward_ma = pd.Series(rewards).rolling(window).mean()
    
    # ì„±ê³µë¥  (ìœ íš¨í•œ í”„ë¡œê·¸ë¨ ë¹„ìœ¨)
    success_rate = 0.1 + 0.8 * (1 - np.exp(-episodes / 150))
    success_noise = np.random.normal(0, 0.05, len(episodes))
    success_rate = np.clip(success_rate + success_noise, 0, 1)
    
    # ìºì‹œ ì ì¤‘ë¥ 
    cache_hit_rate = 0.2 + 0.6 * (1 - np.exp(-episodes / 100))
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('ê°•í™”í•™ìŠµ ì§„í–‰ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜', fontsize=16, fontweight='bold')
    
    # 1. ë³´ìƒ ë³€í™”
    ax1 = axes[0, 0]
    ax1.plot(episodes, rewards, alpha=0.3, color='blue', label='Raw Reward')
    ax1.plot(episodes, reward_ma, color='red', linewidth=2, label=f'{window}-Episode MA')
    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Average Reward')
    ax1.set_title('ë³´ìƒ ì§„í™”')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì„±ê³µë¥  ë³€í™”
    ax2 = axes[0, 1]
    ax2.plot(episodes, success_rate * 100, color='green', linewidth=2)
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Success Rate (%)')
    ax2.set_title('ìœ íš¨í•œ í”„ë¡œê·¸ë¨ ìƒì„± ë¹„ìœ¨')
    ax2.grid(True, alpha=0.3)
    
    # 3. ìºì‹œ ì ì¤‘ë¥ 
    ax3 = axes[1, 0]
    ax3.plot(episodes, cache_hit_rate * 100, color='purple', linewidth=2)
    ax3.set_xlabel('Episode')
    ax3.set_ylabel('Cache Hit Rate (%)')
    ax3.set_title('ìºì‹œ ì ì¤‘ë¥  (íƒìƒ‰ íš¨ìœ¨ì„±)')
    ax3.grid(True, alpha=0.3)
    
    # 4. í˜ë„í‹° êµ¬ì„± ìš”ì†Œ ë³€í™”
    ax4 = axes[1, 1]
    depth_penalty = 0.005 * np.exp(-episodes / 300)  # ê¹Šì´ í˜ë„í‹° ê°ì†Œ
    turnover_penalty = 0.003 * np.exp(-episodes / 250)  # ê±°ë˜ í˜ë„í‹° ê°ì†Œ
    const_penalty = 0.4 * np.exp(-episodes / 180)  # ìƒìˆ˜ í˜ë„í‹° ê°ì†Œ
    
    ax4.plot(episodes, depth_penalty, label='Depth Penalty', alpha=0.8)
    ax4.plot(episodes, turnover_penalty, label='Turnover Penalty', alpha=0.8)
    ax4.plot(episodes, const_penalty, label='Constant Penalty', alpha=0.8)
    ax4.set_xlabel('Episode')
    ax4.set_ylabel('Average Penalty')
    ax4.set_title('í˜ë„í‹° êµ¬ì„± ìš”ì†Œ ë³€í™”')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ì €ì¥
    output_dir = Path("reward_analysis")
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / "learning_progress.png", dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š í•™ìŠµ ì§„í–‰ ì°¨íŠ¸ ì €ì¥: {output_dir / 'learning_progress.png'}")
    
    return fig

if __name__ == "__main__":
    print("ğŸš€ Factor Factory ê°•í™”í•™ìŠµ ë³´ìƒ ì‹œìŠ¤í…œ ìƒì„¸ ë¶„ì„ ì‹œì‘\n")
    
    # 1. ë³´ìƒ êµ¬ì„± ìš”ì†Œ ë¶„ì„
    scenarios, config = analyze_reward_components()
    
    # 2. íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„
    print(f"\nğŸ“ˆ ë³´ìƒ í•¨ìˆ˜ ë¯¼ê°ë„ ë¶„ì„ ì¤‘...")
    plot_reward_sensitivity()
    
    # 3. í˜„ì‹¤ì  ê±°ë˜ íƒ€ì´ë° ì‹œê°í™”
    print(f"\nâ° í˜„ì‹¤ì  ê±°ë˜ íƒ€ì´ë° ë¶„ì„ ì¤‘...")
    plot_realistic_trading_timeline()
    
    # 4. í•™ìŠµ ì§„í–‰ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜
    print(f"\nğŸ¯ í•™ìŠµ ì§„í–‰ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
    generate_learning_progress_simulation()
    
    print(f"\nâœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ 'reward_analysis/' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - reward_sensitivity.png: íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„")
    print(f"   - realistic_trading_timeline.png: í˜„ì‹¤ì  ê±°ë˜ íƒ€ì´ë°")  
    print(f"   - learning_progress.png: í•™ìŠµ ì§„í–‰ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜")
