# ğŸ¯ Factor Factory ë³´ìƒ ì‹œìŠ¤í…œ ê°œì„  ê°€ì´ë“œ

## ğŸ“‹ í˜„ì¬ ë³´ìƒ êµ¬ì¡° ë¶„ì„

### ê¸°ì¡´ ë³´ìƒ í•¨ìˆ˜
```python
reward = (
    pnl_sum                    # ì£¼ ë³´ìƒ: ë°±í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ 
    - lambda_depth * depth     # í˜ë„í‹°: ë³µì¡ë„
    - lambda_turnover * trades # í˜ë„í‹°: ê±°ë˜íšŸìˆ˜
    - lambda_const1 * const_ratio # í˜ë„í‹°: ìƒìˆ˜ ì‚¬ìš©
    - lambda_std / signal_std  # í˜ë„í‹°: ë³€ë™ì„± ë¶€ì¡±
    - signal_quality_penalty   # í˜ë„í‹°: ì‹ í˜¸ í’ˆì§ˆ
)
```

### í˜„ì¬ ì„¤ì •ê°’ ë¬¸ì œì 
```python
lambda_depth = 0.002      # ë„ˆë¬´ ì‘ìŒ â†’ ê³¼ë³µì¡í™” ë°©ì§€ íš¨ê³¼ ë¯¸ë¯¸
lambda_turnover = 0.0005  # ë„ˆë¬´ ì‘ìŒ â†’ ê³¼ê±°ë˜ ì–µì œ íš¨ê³¼ ë¶€ì¡±
lambda_const1 = 2.0       # ë„ˆë¬´ í¼ â†’ ìƒìˆ˜ ì‚¬ìš©ì„ ê³¼ë„í•˜ê²Œ ì–µì œ
lambda_std = 0.5          # ë¶€ì ì ˆ â†’ ë³€ë™ì„±ì´ ì‘ì„ìˆ˜ë¡ í° í˜ë„í‹°
```

---

## ğŸš€ ë³´ìƒ ì‹œìŠ¤í…œ ê°œì„  ë°©ì•ˆ

### 1ï¸âƒ£ **ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ**

#### A. Sharpe Ratio ê¸°ë°˜ ë³´ìƒ
```python
def sharpe_based_reward(pnl, risk_free_rate=0.0):
    """ìƒ¤í”„ ë¹„ìœ¨ì„ ì£¼ ë³´ìƒìœ¼ë¡œ ì‚¬ìš©"""
    returns = pnl
    excess_returns = returns - risk_free_rate
    sharpe = excess_returns.mean() / (returns.std() + 1e-8)
    
    # ìŠ¤ì¼€ì¼ë§: [-2, 2] ë²”ìœ„ë¡œ ì¡°ì •
    return np.tanh(sharpe * 2.0)

# ì¥ì : ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµë¥  ìµœì í™”
# ë‹¨ì : ë³€ë™ì„±ì´ ë‚®ì€ ì „ëµì— ê³¼ë„í•œ ë³´ìƒ
```

#### B. Calmar Ratio ê¸°ë°˜ ë³´ìƒ
```python
def calmar_based_reward(pnl, equity):
    """ì¹¼ë§ˆ ë¹„ìœ¨ ê¸°ë°˜ ë³´ìƒ (MDD ê³ ë ¤)"""
    annual_return = (equity.iloc[-1] - 1) * (252 * 24 / len(pnl))
    
    # ìµœëŒ€ ë‚™í­ ê³„ì‚°
    cummax = equity.cummax()
    drawdown = (equity - cummax) / cummax
    max_drawdown = abs(drawdown.min())
    
    if max_drawdown < 1e-6:
        return 10.0  # ë¬´ì†ì‹¤ ì „ëµì— ë†’ì€ ë³´ìƒ
    
    calmar = annual_return / max_drawdown
    return np.tanh(calmar * 0.5)  # ìŠ¤ì¼€ì¼ë§
```

### 2ï¸âƒ£ **ë‹¤ëª©ì  ìµœì í™” ë³´ìƒ**

#### A. ê°€ì¤‘ í‰ê·  ë°©ì‹
```python
def multi_objective_reward(pnl, equity, signal, trades):
    """ì—¬ëŸ¬ ëª©ì í•¨ìˆ˜ì˜ ê°€ì¤‘ í‰ê· """
    
    # 1. ìˆ˜ìµì„± (30%)
    total_return = (equity.iloc[-1] - 1) * 100
    return_score = np.tanh(total_return / 10)  # Â±10% ê¸°ì¤€
    
    # 2. ìœ„í—˜ì„± (25%)
    volatility = pnl.std() * np.sqrt(252 * 24) * 100
    risk_score = max(0, 1 - volatility / 20)  # 20% ë³€ë™ì„± ê¸°ì¤€
    
    # 3. ì•ˆì •ì„± (25%)
    max_dd = abs((equity / equity.cummax() - 1).min())
    stability_score = max(0, 1 - max_dd / 0.2)  # 20% MDD ê¸°ì¤€
    
    # 4. íš¨ìœ¨ì„± (20%)
    win_rate = (pnl > 0).sum() / len(pnl)
    efficiency_score = win_rate * 2 - 1  # [0,1] â†’ [-1,1]
    
    return (0.3 * return_score + 
            0.25 * risk_score + 
            0.25 * stability_score + 
            0.2 * efficiency_score)
```

#### B. íŒŒë ˆí†  ìµœì í™” ë°©ì‹
```python
def pareto_reward(pnl, equity, signal):
    """íŒŒë ˆí†  í”„ë¡ í‹°ì–´ ê¸°ë°˜ ë³´ìƒ"""
    
    # ëª©ì í•¨ìˆ˜ ì •ì˜
    objectives = {
        'return': (equity.iloc[-1] - 1) * 100,
        'sharpe': pnl.mean() / (pnl.std() + 1e-8),
        'max_dd': -abs((equity / equity.cummax() - 1).min()),
        'win_rate': (pnl > 0).sum() / len(pnl)
    }
    
    # ì •ê·œí™”
    normalized = {}
    for key, value in objectives.items():
        if key == 'return':
            normalized[key] = np.tanh(value / 5)  # Â±5% ê¸°ì¤€
        elif key == 'sharpe':
            normalized[key] = np.tanh(value * 2)
        elif key == 'max_dd':
            normalized[key] = np.tanh(value * 10)  # -10% â†’ -1
        elif key == 'win_rate':
            normalized[key] = value * 2 - 1
    
    # ê°€ì¤‘ ê¸°í•˜í‰ê·  (ëª¨ë“  ëª©í‘œê°€ ê· í˜•)
    weights = [0.3, 0.3, 0.2, 0.2]
    values = list(normalized.values())
    
    # ìŒìˆ˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜¤í”„ì…‹
    offset_values = [v + 2 for v in values]  # [-1,1] â†’ [1,3]
    geo_mean = np.prod([v**w for v, w in zip(offset_values, weights)])
    
    return geo_mean - 2  # ë‹¤ì‹œ [-1,1] ë²”ìœ„ë¡œ
```

### 3ï¸âƒ£ **ë™ì  ë³´ìƒ ì‹œìŠ¤í…œ**

#### A. ì ì‘ì  í˜ë„í‹° ê°€ì¤‘ì¹˜
```python
class AdaptiveRewardSystem:
    def __init__(self):
        self.episode_count = 0
        self.performance_history = []
        
    def get_reward(self, pnl, equity, signal, tokens, depth, trades):
        """ì—í”¼ì†Œë“œ ì§„í–‰ì— ë”°ë¼ ë³´ìƒ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        
        base_reward = pnl.sum()
        
        # ì´ˆê¸°: íƒìƒ‰ ê²©ë ¤ (ë‚®ì€ í˜ë„í‹°)
        if self.episode_count < 1000:
            alpha_complexity = 0.001   # ë³µì¡ë„ í˜ë„í‹° ì•½í•¨
            alpha_turnover = 0.0001    # ê±°ë˜ í˜ë„í‹° ì•½í•¨
            alpha_diversity = -0.01    # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
            
        # ì¤‘ê¸°: ê· í˜• (ì¤‘ê°„ í˜ë„í‹°)
        elif self.episode_count < 5000:
            alpha_complexity = 0.005
            alpha_turnover = 0.001
            alpha_diversity = 0.0
            
        # í›„ê¸°: ìˆ˜ë ´ (ê°•í•œ í˜ë„í‹°)
        else:
            alpha_complexity = 0.01    # ë³µì¡ë„ ê°•í•˜ê²Œ ì–µì œ
            alpha_turnover = 0.005     # ê³¼ê±°ë˜ ê°•í•˜ê²Œ ì–µì œ
            alpha_diversity = 0.005    # ê³¼ì í•© ë°©ì§€
        
        reward = (base_reward 
                 - alpha_complexity * depth
                 - alpha_turnover * trades
                 + alpha_diversity * self._diversity_bonus(tokens))
        
        self.episode_count += 1
        return reward
```

#### B. ìƒëŒ€ì  ìˆœìœ„ ë³´ìƒ
```python
class RankingRewardSystem:
    def __init__(self, buffer_size=1000):
        self.performance_buffer = []
        self.buffer_size = buffer_size
        
    def get_reward(self, pnl, equity, signal):
        """ìµœê·¼ ì„±ê³¼ ëŒ€ë¹„ ìƒëŒ€ì  ìˆœìœ„ë¡œ ë³´ìƒ ê³„ì‚°"""
        
        current_performance = {
            'total_return': equity.iloc[-1] - 1,
            'sharpe': pnl.mean() / (pnl.std() + 1e-8),
            'max_dd': abs((equity / equity.cummax() - 1).min())
        }
        
        # ë²„í¼ì— ì¶”ê°€
        self.performance_buffer.append(current_performance)
        if len(self.performance_buffer) > self.buffer_size:
            self.performance_buffer.pop(0)
        
        if len(self.performance_buffer) < 50:
            return 0.0  # ì¶©ë¶„í•œ ë¹„êµêµ°ì´ ì—†ìœ¼ë©´ ì¤‘ë¦½
        
        # ê° ì§€í‘œë³„ ìƒëŒ€ ìˆœìœ„ ê³„ì‚°
        ranks = {}
        for metric in current_performance:
            values = [p[metric] for p in self.performance_buffer]
            current_value = current_performance[metric]
            
            if metric == 'max_dd':  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                rank = sum(1 for v in values if v > current_value)
            else:  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                rank = sum(1 for v in values if v < current_value)
            
            ranks[metric] = rank / len(values)  # [0,1] ì •ê·œí™”
        
        # ê°€ì¤‘ í‰ê·  ìˆœìœ„
        final_rank = (0.4 * ranks['total_return'] + 
                     0.4 * ranks['sharpe'] + 
                     0.2 * ranks['max_dd'])
        
        return (final_rank - 0.5) * 4  # [0,1] â†’ [-2,2]
```

### 4ï¸âƒ£ **ì‹¤ìš©ì  ê°œì„  ë°©ì•ˆ**

#### A. ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ë³´ìƒ ì¡°ì •
```python
# í˜„ì¬ env.pyì—ì„œ ìˆ˜ì • ê°€ëŠ¥í•œ ë¶€ë¶„
@dataclass
class ImprovedRLCConfig:
    # ê¸°ì¡´ ì„¤ì •
    max_len: int = 21
    
    # ê°œì„ ëœ í˜ë„í‹° ê°€ì¤‘ì¹˜
    lambda_depth: float = 0.01        # 5ë°° ì¦ê°€: ë³µì¡ë„ ê°•ë ¥ ì–µì œ
    lambda_turnover: float = 0.002    # 4ë°° ì¦ê°€: ê³¼ê±°ë˜ ì–µì œ
    lambda_const1: float = 0.5        # 1/4ë¡œ ê°ì†Œ: ìƒìˆ˜ ì‚¬ìš© í—ˆìš©
    lambda_std: float = 0.1           # 1/5ë¡œ ê°ì†Œ: ë³€ë™ì„± í˜ë„í‹° ì™„í™”
    
    # ìƒˆë¡œìš´ í˜ë„í‹°
    lambda_drawdown: float = 2.0      # MDD í˜ë„í‹° ì¶”ê°€
    lambda_consistency: float = 0.5   # ì¼ê´€ì„± í˜ë„í‹° ì¶”ê°€
    lambda_skewness: float = 0.1      # í¸í–¥ì„± í˜ë„í‹° ì¶”ê°€
    
    # ë™ì  ê°€ì¤‘ì¹˜ í™œì„±í™”
    adaptive_weights: bool = True
    performance_window: int = 100     # ì„±ê³¼ ë¹„êµ ìœˆë„ìš°
```

#### B. ë³´ìƒ ì •ê·œí™” ë° í´ë¦¬í•‘
```python
def normalized_reward(raw_reward, history_window=1000):
    """ë³´ìƒì˜ ì •ê·œí™” ë° í´ë¦¬í•‘"""
    
    # ì—­ì‚¬ì  ë³´ìƒ ì¶”ì 
    if not hasattr(normalized_reward, 'reward_history'):
        normalized_reward.reward_history = []
    
    normalized_reward.reward_history.append(raw_reward)
    if len(normalized_reward.reward_history) > history_window:
        normalized_reward.reward_history.pop(0)
    
    # Z-score ì •ê·œí™”
    if len(normalized_reward.reward_history) > 10:
        history = np.array(normalized_reward.reward_history)
        mean = history.mean()
        std = history.std() + 1e-8
        normalized = (raw_reward - mean) / std
    else:
        normalized = raw_reward
    
    # í´ë¦¬í•‘ [-3, 3]
    clipped = np.clip(normalized, -3.0, 3.0)
    
    # íƒ„ì  íŠ¸ ìŠ¤ì¼€ì¼ë§ [-1, 1]
    final_reward = np.tanh(clipped)
    
    return final_reward
```

### 5ï¸âƒ£ **ì‹¤í—˜ì  ë³´ìƒ ì‹œìŠ¤í…œ**

#### A. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ë³´ìƒ
```python
class CurriculumReward:
    def __init__(self):
        self.phase = 'exploration'  # exploration â†’ exploitation â†’ refinement
        self.episode_count = 0
        
    def get_reward(self, pnl, equity, signal, tokens):
        self.episode_count += 1
        
        # Phase 1: íƒìƒ‰ (0-2000 ì—í”¼ì†Œë“œ)
        if self.episode_count <= 2000:
            self.phase = 'exploration'
            return self._exploration_reward(pnl, tokens)
            
        # Phase 2: í™œìš© (2000-8000 ì—í”¼ì†Œë“œ)
        elif self.episode_count <= 8000:
            self.phase = 'exploitation'
            return self._exploitation_reward(pnl, equity, signal)
            
        # Phase 3: ì •ì œ (8000+ ì—í”¼ì†Œë“œ)
        else:
            self.phase = 'refinement'
            return self._refinement_reward(pnl, equity, signal, tokens)
    
    def _exploration_reward(self, pnl, tokens):
        """íƒìƒ‰ ë‹¨ê³„: ë‹¤ì–‘ì„± ê²©ë ¤"""
        base = pnl.sum()
        diversity_bonus = len(set(tokens)) * 0.1  # í† í° ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        return base + diversity_bonus
    
    def _exploitation_reward(self, pnl, equity, signal):
        """í™œìš© ë‹¨ê³„: ìˆ˜ìµì„± ì¤‘ì‹¬"""
        return pnl.sum() * 2.0  # ìˆ˜ìµë¥ ì— ê°€ì¤‘ì¹˜
    
    def _refinement_reward(self, pnl, equity, signal, tokens):
        """ì •ì œ ë‹¨ê³„: ì•ˆì •ì„± + ë‹¨ìˆœí•¨"""
        base = pnl.sum()
        stability = -abs((equity / equity.cummax() - 1).min())
        simplicity = -len(tokens) * 0.01
        return base + stability + simplicity
```

#### B. ê°•í™”í•™ìŠµ ë©”íƒ€ ë³´ìƒ
```python
def meta_reward(pnl, equity, signal, episode_reward_history):
    """ë©”íƒ€ í•™ìŠµ ê¸°ë°˜ ë³´ìƒ ì¡°ì •"""
    
    # ê¸°ë³¸ ë³´ìƒ
    base_reward = pnl.sum()
    
    # ìµœê·¼ 100 ì—í”¼ì†Œë“œ ì„±ê³¼ íŠ¸ë Œë“œ
    if len(episode_reward_history) >= 100:
        recent_trend = np.polyfit(range(100), episode_reward_history[-100:], 1)[0]
        
        # ì„±ê³¼ í–¥ìƒ ì¤‘ì´ë©´ ë³´ë„ˆìŠ¤
        improvement_bonus = max(0, recent_trend * 10)
        
        # ì„±ê³¼ ì •ì²´ ì‹œ íƒìƒ‰ ê²©ë ¤
        if abs(recent_trend) < 0.01:
            exploration_bonus = 0.1
        else:
            exploration_bonus = 0.0
            
        return base_reward + improvement_bonus + exploration_bonus
    
    return base_reward
```

---

## ğŸ¯ ê¶Œì¥ êµ¬í˜„ ìˆœì„œ

### 1ë‹¨ê³„: ì¦‰ì‹œ ê°œì„  (Low Risk)
```python
# env.pyì—ì„œ í˜„ì¬ ê°€ì¤‘ì¹˜ë§Œ ì¡°ì •
lambda_depth: float = 0.01        # í˜„ì¬ 0.002 â†’ 0.01
lambda_turnover: float = 0.002    # í˜„ì¬ 0.0005 â†’ 0.002  
lambda_const1: float = 0.5        # í˜„ì¬ 2.0 â†’ 0.5
```

### 2ë‹¨ê³„: ìœ„í—˜ ì¡°ì • ë³´ìƒ ì¶”ê°€ (Medium Risk)
```python
# Sharpe ratio ê¸°ë°˜ ë³´ìƒìœ¼ë¡œ ë³€ê²½
reward = sharpe_ratio * 2.0 - penalties
```

### 3ë‹¨ê³„: ë‹¤ëª©ì  ë³´ìƒ ë„ì… (High Risk)
```python
# ë‹¤ëª©ì  ìµœì í™” ë³´ìƒ ì‹œìŠ¤í…œ êµ¬í˜„
reward = multi_objective_reward(pnl, equity, signal, trades)
```

### 4ë‹¨ê³„: ë™ì  ì‹œìŠ¤í…œ êµ¬í˜„ (Experimental)
```python
# ì ì‘ì  ë˜ëŠ” ì»¤ë¦¬í˜ëŸ¼ ë³´ìƒ ì‹œìŠ¤í…œ
reward_system = AdaptiveRewardSystem()
reward = reward_system.get_reward(...)
```

ê° ë‹¨ê³„ë³„ë¡œ ì‹¤í—˜í•˜ë©´ì„œ ì„±ëŠ¥ ë³€í™”ë¥¼ ê´€ì°°í•˜ê³ , ê°€ì¥ íš¨ê³¼ì ì¸ ì¡°í•©ì„ ì°¾ì•„ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤! ğŸš€
