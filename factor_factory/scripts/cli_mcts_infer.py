#!/usr/bin/env python3
"""
MCTS ê¸°ë°˜ Factor Inference CLI

í•™ìŠµëœ MCTS ëª¨ë¸ë¡œ ìµœì  íŒ©í„° íƒìƒ‰ ë° ì¶”ë¡ 
"""

import argparse
import logging
import sys
import json
import time
from pathlib import Path
import numpy as np
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

from factor_factory.data import ParquetCache, DATA_ROOT
from factor_factory.rlc import RLCConfig
from factor_factory.rlc.utils import tokens_to_infix, calc_tree_depth
from factor_factory.mcts import (
    PolicyValueNetwork, 
    MCTSFactorEnv, 
    MCTSSearch,
    AdaptiveMCTS
)
from factor_factory.pool import FactorPool


def mcts_inference(
    network: PolicyValueNetwork,
    env: MCTSFactorEnv,
    num_searches: int = 10,
    mcts_simulations: int = 800,
    temperature: float = 0.1,
    c_puct: float = 1.0
):
    """MCTS ê¸°ë°˜ ì¶”ë¡ """
    
    # MCTS íƒìƒ‰ê¸° ìƒì„±
    mcts = MCTSSearch(
        network=network,
        c_puct=c_puct,
        num_simulations=mcts_simulations,
        evaluation_fn=lambda tokens: env.evaluate_program(tokens)['reward']
    )
    
    best_programs = []
    
    for search_idx in range(num_searches):
        logging.info(f"ğŸ” íƒìƒ‰ {search_idx + 1}/{num_searches}")
        
        try:
            # MCTS íƒìƒ‰ ì‹¤í–‰
            action_probs, root_node = mcts.search(root_state=[], root_need=1)
            
            # í”„ë¡œê·¸ë¨ ìƒì„±
            program = generate_program_from_root(root_node, temperature)
            
            if program:
                # í”„ë¡œê·¸ë¨ í‰ê°€
                evaluation = env.evaluate_program(program)
                
                if evaluation['success']:
                    formula = tokens_to_infix(program)
                    depth = calc_tree_depth(program)
                    
                    result = {
                        'search_idx': search_idx,
                        'program': program,
                        'formula': formula,
                        'reward': evaluation['reward'],
                        'depth': depth,
                        'length': len(program),
                        'evaluation': evaluation,
                        'mcts_stats': {
                            'total_visits': root_node.visit_count,
                            'best_action': mcts.get_best_action(root_node),
                            'principal_variation': mcts.get_principal_variation(root_node)
                        }
                    }
                    
                    best_programs.append(result)
                    
                    logging.info(f"âœ… í”„ë¡œê·¸ë¨ ë°œê²¬: reward={evaluation['reward']:.4f}")
                    logging.info(f"   ê³µì‹: {formula}")
                    logging.info(f"   ê¹Šì´: {depth}, ê¸¸ì´: {len(program)}")
                else:
                    logging.info(f"âŒ í”„ë¡œê·¸ë¨ í‰ê°€ ì‹¤íŒ¨: {evaluation.get('error', 'unknown')}")
            else:
                logging.info(f"âŒ í”„ë¡œê·¸ë¨ ìƒì„± ì‹¤íŒ¨")
                
        except Exception as e:
            logging.error(f"íƒìƒ‰ {search_idx} ì˜¤ë¥˜: {e}")
            continue
    
    return best_programs


def generate_program_from_root(root_node, temperature: float = 0.1):
    """ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì™„ì „í•œ í”„ë¡œê·¸ë¨ ìƒì„±"""
    
    program = []
    node = root_node
    max_steps = 21
    
    for step in range(max_steps):
        if not node.children or node.is_terminal:
            break
        
        # ì˜¨ë„ ì¡°ì ˆëœ ì•¡ì…˜ ì„ íƒ
        action_probs = node.get_action_probs(temperature)
        valid_actions = list(node.children.keys())
        
        if not valid_actions:
            break
        
        # í™•ë¥  ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ
        if temperature == 0:
            # Greedy ì„ íƒ
            action = max(valid_actions, key=lambda a: node.children[a].visit_count)
        else:
            # í™•ë¥ ì  ì„ íƒ
            valid_probs = np.array([action_probs[a] for a in valid_actions])
            if valid_probs.sum() > 0:
                valid_probs = valid_probs / valid_probs.sum()
                action_idx = np.random.choice(len(valid_actions), p=valid_probs)
                action = valid_actions[action_idx]
            else:
                action = np.random.choice(valid_actions)
        
        program.append(action)
        
        if action in node.children:
            node = node.children[action]
        else:
            break
    
    return program if program else None


def compare_with_ppo_results(mcts_results, ppo_results_path):
    """PPO ê²°ê³¼ì™€ ë¹„êµ"""
    
    if not Path(ppo_results_path).exists():
        logging.warning(f"PPO ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {ppo_results_path}")
        return
    
    try:
        with open(ppo_results_path, 'r') as f:
            ppo_data = json.load(f)
        
        ppo_best_reward = ppo_data.get('best_reward', 0)
        ppo_best_formula = ppo_data.get('best_formula', 'N/A')
        
        # MCTS ìµœê³  ê²°ê³¼
        if mcts_results:
            mcts_best = max(mcts_results, key=lambda x: x['reward'])
            mcts_best_reward = mcts_best['reward']
            mcts_best_formula = mcts_best['formula']
            
            logging.info(f"\nğŸ” PPO vs MCTS ë¹„êµ:")
            logging.info(f"PPO  ìµœê³  ë³´ìƒ: {ppo_best_reward:.4f} | {ppo_best_formula}")
            logging.info(f"MCTS ìµœê³  ë³´ìƒ: {mcts_best_reward:.4f} | {mcts_best_formula}")
            
            if mcts_best_reward > ppo_best_reward:
                logging.info(f"ğŸ† MCTSê°€ PPOë³´ë‹¤ {mcts_best_reward - ppo_best_reward:.4f} ìš°ìˆ˜!")
            else:
                logging.info(f"ğŸ“Š PPOê°€ MCTSë³´ë‹¤ {ppo_best_reward - mcts_best_reward:.4f} ìš°ìˆ˜")
    
    except Exception as e:
        logging.error(f"PPO ê²°ê³¼ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")


def main(argv=None):
    parser = argparse.ArgumentParser(
        description="MCTS ê¸°ë°˜ Factor Inference"
    )
    
    # í•„ìˆ˜ ì¸ì
    parser.add_argument("--model", required=True, 
                       help="í•™ìŠµëœ MCTS ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--symbol", required=True, 
                       help="ì‹¬ë³¼ (ì˜ˆ: BTCUSDT)")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--interval", default="1h", help="ì‹œê°„ ê°„ê²©")
    
    # ì¶”ë¡  ì„¤ì •
    parser.add_argument("--num-searches", type=int, default=20,
                       help="íƒìƒ‰ íšŸìˆ˜")
    parser.add_argument("--mcts-simulations", type=int, default=1000,
                       help="MCTS ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜")
    parser.add_argument("--temperature", type=float, default=0.1,
                       help="ì•¡ì…˜ ì„ íƒ ì˜¨ë„ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ greedy)")
    parser.add_argument("--c-puct", type=float, default=1.0,
                       help="UCB íƒìƒ‰ ìƒìˆ˜")
    parser.add_argument("--use-adaptive", action="store_true",
                       help="ì ì‘ì  MCTS ì‚¬ìš©")
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument("--max-len", type=int, default=21)
    parser.add_argument("--eval-stride", type=int, default=2)
    parser.add_argument("--max-eval-bars", type=int, default=20000)
    parser.add_argument("--long-threshold", type=float, default=1.5)
    parser.add_argument("--short-threshold", type=float, default=-1.5)
    parser.add_argument("--rolling-window", type=int, default=252)
    parser.add_argument("--commission", type=float, default=0.0008)
    parser.add_argument("--slippage", type=float, default=0.0015)
    parser.add_argument("--leverage", type=float, default=1.0)
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument("--outdir", default="mcts_inference_results",
                       help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--factor-pool-dir", 
                       help="ìš°ìˆ˜í•œ íŒ©í„°ë¥¼ ì €ì¥í•  Factor Pool ë””ë ‰í† ë¦¬")
    parser.add_argument("--compare-ppo", 
                       help="PPO ê²°ê³¼ì™€ ë¹„êµí•  JSON íŒŒì¼ ê²½ë¡œ")
    
    # ê¸°íƒ€
    parser.add_argument("--device", default="auto")
    parser.add_argument("--verbose", action="store_true")
    
    args = parser.parse_args(argv)
    
    # â”€â”€ ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    outdir = Path(args.outdir)
    outdir.mkdir(exist_ok=True, parents=True)
    
    # â”€â”€ ë°ì´í„° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logging.info(f"ë°ì´í„° ë¡œë”©: {args.symbol}_{args.interval}")
    cache = ParquetCache(DATA_ROOT)
    
    try:
        df = cache.load(args.symbol, args.interval)
        logging.info(f"ë°ì´í„° í¬ê¸°: {df.shape}, ê¸°ê°„: {df.index[0]} ~ {df.index[-1]}")
    except Exception as e:
        logging.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        return
    
    # â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    rlc_config = RLCConfig(
        max_len=args.max_len,
        eval_stride=args.eval_stride,
        max_eval_bars=args.max_eval_bars,
        long_threshold=args.long_threshold,
        short_threshold=args.short_threshold,
        rolling_window=args.rolling_window,
        commission=args.commission,
        slippage=args.slippage,
        leverage=args.leverage
    )
    
    mcts_env = MCTSFactorEnv(df, rlc_config)
    
    # â”€â”€ ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logging.info(f"MCTS ëª¨ë¸ ë¡œë”©: {args.model}")
    
    try:
        network = PolicyValueNetwork(
            input_dim=23,
            action_dim=25
        )
        
        # ëª¨ë¸ ë¡œë”© ë°©ì‹ ê²°ì •
        model_path = Path(args.model)
        if model_path.suffix == '.pt':
            # PyTorch ì²´í¬í¬ì¸íŠ¸
            import torch
            checkpoint = torch.load(args.model, map_location='cpu')
            network.load_state_dict(checkpoint['network_state_dict'])
        else:
            logging.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í˜•ì‹: {model_path.suffix}")
            return
        
        logging.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    except Exception as e:
        logging.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return
    
    # â”€â”€ MCTS ì¶”ë¡  ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logging.info(f"ğŸš€ MCTS ì¶”ë¡  ì‹œì‘")
    logging.info(f"ì„¤ì •: {args.num_searches}íšŒ íƒìƒ‰, {args.mcts_simulations} ì‹œë®¬ë ˆì´ì…˜")
    
    start_time = time.time()
    
    try:
        results = mcts_inference(
            network=network,
            env=mcts_env,
            num_searches=args.num_searches,
            mcts_simulations=args.mcts_simulations,
            temperature=args.temperature,
            c_puct=args.c_puct
        )
        
        inference_time = time.time() - start_time
        
        # â”€â”€ ê²°ê³¼ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if results:
            # ë³´ìƒ ê¸°ì¤€ ì •ë ¬
            results.sort(key=lambda x: x['reward'], reverse=True)
            
            logging.info(f"\nğŸ† ìƒìœ„ ê²°ê³¼:")
            for i, result in enumerate(results[:5]):
                logging.info(f"{i+1}. ë³´ìƒ: {result['reward']:.4f} | {result['formula']}")
            
            # í†µê³„
            rewards = [r['reward'] for r in results]
            depths = [r['depth'] for r in results]
            lengths = [r['length'] for r in results]
            
            stats = {
                'total_programs': len(results),
                'avg_reward': np.mean(rewards),
                'max_reward': np.max(rewards),
                'min_reward': np.min(rewards),
                'avg_depth': np.mean(depths),
                'avg_length': np.mean(lengths),
                'inference_time': inference_time,
                'success_rate': len(results) / args.num_searches
            }
            
            logging.info(f"\nğŸ“Š í†µê³„:")
            logging.info(f"  ì„±ê³µí•œ í”„ë¡œê·¸ë¨: {stats['total_programs']}/{args.num_searches}")
            logging.info(f"  í‰ê·  ë³´ìƒ: {stats['avg_reward']:.4f}")
            logging.info(f"  ìµœê³  ë³´ìƒ: {stats['max_reward']:.4f}")
            logging.info(f"  í‰ê·  ê¹Šì´: {stats['avg_depth']:.1f}")
            logging.info(f"  ì¶”ë¡  ì‹œê°„: {stats['inference_time']:.1f}ì´ˆ")
            
        else:
            logging.warning("ì„±ê³µí•œ í”„ë¡œê·¸ë¨ì´ ì—†ìŠµë‹ˆë‹¤.")
            stats = {'total_programs': 0, 'success_rate': 0}
        
        # â”€â”€ ê²°ê³¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        results_data = {
            'config': vars(args),
            'stats': stats,
            'programs': results,
            'timestamp': time.time()
        }
        
        results_file = outdir / f"mcts_inference_{args.symbol}_{int(time.time())}.json"
        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2, default=str)
        
        logging.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # â”€â”€ Factor Pool ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if args.factor_pool_dir and results:
            logging.info(f"Factor Poolì— ìš°ìˆ˜í•œ íŒ©í„° ì €ì¥...")
            
            factor_pool = FactorPool(args.factor_pool_dir)
            saved_count = 0
            
            # ìƒìœ„ 5ê°œ í”„ë¡œê·¸ë¨ ì €ì¥
            for result in results[:5]:
                if result['reward'] > 0:  # ì–‘ìˆ˜ ë³´ìƒë§Œ
                    try:
                        # ê°€ìƒ ì‹œê³„ì—´ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì‚¬ìš©)
                        import pandas as pd
                        dates = pd.date_range('2024-01-01', periods=1000, freq='H')
                        pnl = pd.Series(np.random.normal(result['reward']/1000, 0.01, 1000), index=dates)
                        equity = (1 + pnl).cumprod()
                        signal = pd.Series(np.random.normal(0, 1, 1000), index=dates)
                        
                        reward_info = {
                            'total_reward': result['reward'],
                            'components': {'mcts_inference': result['reward']},
                            'future_leak': False,
                            'validation': {'mcts_evaluated': True}
                        }
                        
                        factor_pool.add_factor(
                            tokens=result['program'],
                            formula=result['formula'],
                            pnl=pnl,
                            equity=equity,
                            signal=signal,
                            reward_info=reward_info,
                            model_version="MCTS_Inference",
                            training_episode=0
                        )
                        
                        saved_count += 1
                        
                    except Exception as e:
                        logging.warning(f"Factor Pool ì €ì¥ ì‹¤íŒ¨: {e}")
            
            logging.info(f"âœ… {saved_count}ê°œ íŒ©í„° ì €ì¥ ì™„ë£Œ")
        
        # â”€â”€ PPO ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if args.compare_ppo:
            compare_with_ppo_results(results, args.compare_ppo)
        
        # â”€â”€ í™˜ê²½ í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        env_stats = mcts_env.get_statistics()
        logging.info(f"\nğŸ­ í™˜ê²½ í†µê³„: {env_stats}")
        
        logging.info("âœ… MCTS ì¶”ë¡  ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        logging.info("ì‚¬ìš©ìì— ì˜í•´ ì¶”ë¡ ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
