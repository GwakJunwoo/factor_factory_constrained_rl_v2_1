#!/usr/bin/env python3
"""
ì •ì±…-ê°€ì¹˜ ì‹ ê²½ë§ (Policy-Value Network)

AlphaZero ìŠ¤íƒ€ì¼ë¡œ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì •ì±…ê³¼ ê°€ì¹˜ë¥¼ ë™ì‹œì— ì˜ˆì¸¡:
- ì…ë ¥: í˜„ì¬ í† í° ì‹œí€€ìŠ¤ ìƒíƒœ (23ì°¨ì› observation)  
- ì¶œë ¥: ì •ì±… í™•ë¥  ë¶„í¬ (25ì°¨ì›) + ìƒíƒœ ê°€ì¹˜ (1ì°¨ì›)
"""

import numpy as np
from typing import Tuple, Dict, List
import warnings
warnings.filterwarnings('ignore')

# PyTorch ì¡°ê±´ë¶€ import
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‹ ê²½ë§ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("PyTorch ì„¤ì¹˜: pip install torch")


class DummyPolicyValueNetwork:
    """PyTorchê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë”ë¯¸ ì‹ ê²½ë§"""
    
    def __init__(self, *args, **kwargs):
        print("âš ï¸ ë”ë¯¸ ì‹ ê²½ë§ ëª¨ë“œ: ëœë¤ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        self.input_dim = kwargs.get('input_dim', 23)
        self.action_dim = kwargs.get('action_dim', 25)
    
    def predict(self, observation: np.ndarray) -> Tuple[np.ndarray, float]:
        """ëœë¤ ì •ì±…ê³¼ ê°€ì¹˜ ë°˜í™˜"""
        policy_probs = np.random.rand(self.action_dim)
        policy_probs = policy_probs / policy_probs.sum()
        value = np.random.uniform(-1, 1)
        return policy_probs, value
    
    def predict_batch(self, observations: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ë°°ì¹˜ ëœë¤ ì˜ˆì¸¡"""
        batch_size = observations.shape[0]
        policies = np.random.rand(batch_size, self.action_dim)
        policies = policies / policies.sum(axis=1, keepdims=True)
        values = np.random.uniform(-1, 1, batch_size)
        return policies, values
    
    def eval(self):
        """í‰ê°€ ëª¨ë“œ (ë”ë¯¸)"""
        pass
    
    def train(self):
        """í•™ìŠµ ëª¨ë“œ (ë”ë¯¸)"""
        pass


class DummyNetworkTrainer:
    """ë”ë¯¸ ë„¤íŠ¸ì›Œí¬ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, *args, **kwargs):
        self.network = args[0] if args else DummyPolicyValueNetwork()
        print("âš ï¸ ë”ë¯¸ íŠ¸ë ˆì´ë„ˆ ëª¨ë“œ: ì‹¤ì œ í•™ìŠµì€ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def train_step(self, *args, **kwargs):
        return {'total_loss': 0.0, 'policy_loss': 0.0, 'value_loss': 0.0}
    
    def evaluate(self, *args, **kwargs):
        return {'total_loss': 0.0, 'policy_loss': 0.0, 'value_loss': 0.0}
    
    def save_model(self, filepath: str):
        print(f"âš ï¸ ë”ë¯¸ ëª¨ë“œ: ëª¨ë¸ ì €ì¥ ê±´ë„ˆëœ€ ({filepath})")
    
    def load_model(self, filepath: str):
        print(f"âš ï¸ ë”ë¯¸ ëª¨ë“œ: ëª¨ë¸ ë¡œë“œ ê±´ë„ˆëœ€ ({filepath})")


if not TORCH_AVAILABLE:
    # PyTorchê°€ ì—†ìœ¼ë©´ ë”ë¯¸ í´ë˜ìŠ¤ ì‚¬ìš©
    PolicyValueNetwork = DummyPolicyValueNetwork
    NetworkTrainer = DummyNetworkTrainer
    
else:
    # PyTorchê°€ ìˆì„ ë•Œì˜ ì‹¤ì œ êµ¬í˜„
    class PolicyValueNetwork(nn.Module):
        """ì •ì±…-ê°€ì¹˜ ì‹ ê²½ë§"""
        
        def __init__(
            self,
            input_dim: int = 23,
            hidden_dims: List[int] = [256, 256, 128],
            action_dim: int = 25,
            dropout_rate: float = 0.1
        ):
            super().__init__()
            
            self.input_dim = input_dim
            self.action_dim = action_dim
            
            # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬
            layers = []
            prev_dim = input_dim
            
            for hidden_dim in hidden_dims:
                layers.extend([
                    nn.Linear(prev_dim, hidden_dim),
                    nn.ReLU(),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Dropout(dropout_rate)
                ])
                prev_dim = hidden_dim
            
            self.feature_extractor = nn.Sequential(*layers)
            
            # ì •ì±… í—¤ë“œ
            self.policy_head = nn.Sequential(
                nn.Linear(prev_dim, 128),
                nn.ReLU(),
                nn.Linear(128, action_dim)
            )
            
            # ê°€ì¹˜ í—¤ë“œ
            self.value_head = nn.Sequential(
                nn.Linear(prev_dim, 64),
                nn.ReLU(), 
                nn.Linear(64, 1),
                nn.Tanh()
            )
            
            self.apply(self._init_weights)
        
        def _init_weights(self, module):
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
        def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            features = self.feature_extractor(x)
            policy_logits = self.policy_head(features)
            value = self.value_head(features)
            return policy_logits, value
        
        def predict(self, observation: np.ndarray) -> Tuple[np.ndarray, float]:
            self.eval()
            with torch.no_grad():
                x = torch.FloatTensor(observation).unsqueeze(0)
                policy_logits, value = self.forward(x)
                policy_probs = F.softmax(policy_logits, dim=1)
                return policy_probs[0].numpy(), value[0].item()
        
        def predict_batch(self, observations: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
            self.eval()
            with torch.no_grad():
                x = torch.FloatTensor(observations)
                policy_logits, values = self.forward(x)
                policy_probs = F.softmax(policy_logits, dim=1)
                return policy_probs.numpy(), values.squeeze().numpy()
    
    
    class NetworkTrainer:
        """ì‹ ê²½ë§ í•™ìŠµ ê´€ë¦¬ í´ë˜ìŠ¤"""
        
        def __init__(self, network: PolicyValueNetwork, learning_rate: float = 1e-3, 
                     weight_decay: float = 1e-4, device: str = 'auto'):
            if device == 'auto':
                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            else:
                self.device = torch.device(device)
            
            self.network = network.to(self.device)
            self.criterion = nn.MSELoss()  # ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜
            self.optimizer = torch.optim.Adam(
                network.parameters(), lr=learning_rate, weight_decay=weight_decay
            )
            
        def train_step(self, states: np.ndarray, policy_targets: np.ndarray, 
                       value_targets: np.ndarray) -> Dict[str, float]:
            self.network.train()
            
            states = torch.FloatTensor(states).to(self.device)
            policy_targets = torch.FloatTensor(policy_targets).to(self.device)
            value_targets = torch.FloatTensor(value_targets).to(self.device)
            
            policy_logits, value_pred = self.network(states)
            
            # ê°„ë‹¨í•œ ì†ì‹¤ ê³„ì‚°
            policy_loss = -torch.mean(torch.sum(policy_targets * F.log_softmax(policy_logits, dim=1), dim=1))
            value_loss = self.criterion(value_pred.squeeze(), value_targets)
            total_loss = policy_loss + value_loss
            
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            return {
                'total_loss': total_loss.item(),
                'policy_loss': policy_loss.item(),
                'value_loss': value_loss.item()
            }
        
        def evaluate(self, states: np.ndarray, policy_targets: np.ndarray, 
                     value_targets: np.ndarray) -> Dict[str, float]:
            self.network.eval()
            with torch.no_grad():
                states = torch.FloatTensor(states).to(self.device)
                policy_targets = torch.FloatTensor(policy_targets).to(self.device)
                value_targets = torch.FloatTensor(value_targets).to(self.device)
                
                policy_logits, value_pred = self.network(states)
                
                policy_loss = -torch.mean(torch.sum(policy_targets * F.log_softmax(policy_logits, dim=1), dim=1))
                value_loss = self.criterion(value_pred.squeeze(), value_targets)
                
                return {
                    'total_loss': (policy_loss + value_loss).item(),
                    'policy_loss': policy_loss.item(),
                    'value_loss': value_loss.item()
                }
        
        def save_model(self, filepath: str):
            torch.save({
                'network_state_dict': self.network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, filepath)
            print(f"âœ… ëª¨ë¸ ì €ì¥: {filepath}")
        
        def load_model(self, filepath: str):
            checkpoint = torch.load(filepath, map_location=self.device)
            self.network.load_state_dict(checkpoint['network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print(f"âœ… ëª¨ë¸ ë¡œë“œ: {filepath}")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ§  Policy-Value Network í…ŒìŠ¤íŠ¸")
    
    # ë„¤íŠ¸ì›Œí¬ ìƒì„±
    network = PolicyValueNetwork()
    trainer = NetworkTrainer(network)
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    batch_size = 32
    states = np.random.randn(batch_size, 23)
    policy_targets = np.random.rand(batch_size, 25)
    policy_targets = policy_targets / policy_targets.sum(axis=1, keepdims=True)
    value_targets = np.random.uniform(-1, 1, batch_size)
    
    # í•™ìŠµ í…ŒìŠ¤íŠ¸
    print("í•™ìŠµ í…ŒìŠ¤íŠ¸...")
    loss_dict = trainer.train_step(states, policy_targets, value_targets)
    print(f"ì†ì‹¤: {loss_dict}")
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
    single_state = states[0]
    policy_probs, value = network.predict(single_state)
    print(f"ì •ì±… í™•ë¥  ë¶„í¬ í˜•íƒœ: {policy_probs.shape}")
    print(f"ì •ì±… í™•ë¥  í•©: {policy_probs.sum():.3f}")
    print(f"ì˜ˆì¸¡ ê°€ì¹˜: {value:.3f}")
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
