#!/usr/bin/env python3
"""
AlphaZero ìŠ¤íƒ€ì¼ íŠ¸ë ˆì´ë„ˆ

ìê¸° ëŒ€êµ­(Self-Play) + ì‹ ê²½ë§ í•™ìŠµì„ ë°˜ë³µí•˜ëŠ” AlphaZero ì•Œê³ ë¦¬ì¦˜:
1. í˜„ì¬ ì‹ ê²½ë§ìœ¼ë¡œ MCTS ìˆ˜í–‰í•˜ì—¬ í”„ë¡œê·¸ë¨ ìƒì„±
2. ìƒì„±ëœ í”„ë¡œê·¸ë¨ë“¤ì„ í‰ê°€í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
3. ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ì‹ ê²½ë§ ì—…ë°ì´íŠ¸
4. ì£¼ê¸°ì ìœ¼ë¡œ ì´ì „ ë²„ì „ê³¼ ì„±ëŠ¥ ë¹„êµ
"""

import os
import time
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

from .neural_network import PolicyValueNetwork, NetworkTrainer
from .mcts_search import MCTSSearch, AdaptiveMCTS
from .mcts_env import MCTSFactorEnv, MCTSDataCollector
from ..rlc.env import RLCConfig
from ..rlc.utils import tokens_to_infix
from ..pool import FactorPool, AutoSaveFactorCallback


class AlphaZeroTrainer:
    """AlphaZero ìŠ¤íƒ€ì¼ íŒ©í„° ë°œê²¬ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(
        self,
        env: MCTSFactorEnv,
        network: PolicyValueNetwork,
        factor_pool: Optional[FactorPool] = None,
        # MCTS ì„¤ì •
        mcts_simulations: int = 800,
        c_puct: float = 1.0,
        # í•™ìŠµ ì„¤ì •
        episodes_per_iteration: int = 100,
        training_batch_size: int = 512,
        training_epochs: int = 10,
        # ì„±ëŠ¥ í‰ê°€
        evaluation_episodes: int = 50,
        evaluation_interval: int = 5,
        # ì €ì¥ ì„¤ì •
        save_interval: int = 10,
        checkpoint_dir: str = "mcts_checkpoints"
    ):
        """
        Args:
            env: MCTS Factor Environment
            network: Policy-Value Network
            factor_pool: Factor Pool for storing discovered factors
            mcts_simulations: MCTS ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜
            c_puct: UCB íƒìƒ‰ ìƒìˆ˜
            episodes_per_iteration: ë°˜ë³µë‹¹ ì—í”¼ì†Œë“œ ìˆ˜
            training_batch_size: í•™ìŠµ ë°°ì¹˜ í¬ê¸°
            training_epochs: ì‹ ê²½ë§ í•™ìŠµ ì—í¬í¬
            evaluation_episodes: í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜
            evaluation_interval: í‰ê°€ ì£¼ê¸°
            save_interval: ì €ì¥ ì£¼ê¸°
            checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        """
        self.env = env
        self.network = network
        self.factor_pool = factor_pool
        
        # MCTS íƒìƒ‰ê¸°
        self.mcts = MCTSSearch(
            network=network,
            c_puct=c_puct,
            num_simulations=mcts_simulations,
            evaluation_fn=self._evaluate_tokens
        )
        
        # ë„¤íŠ¸ì›Œí¬ íŠ¸ë ˆì´ë„ˆ
        self.trainer = NetworkTrainer(network)
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸°
        self.data_collector = MCTSDataCollector(max_samples=50000)
        
        # í•™ìŠµ ì„¤ì •
        self.episodes_per_iteration = episodes_per_iteration
        self.training_batch_size = training_batch_size
        self.training_epochs = training_epochs
        self.evaluation_episodes = evaluation_episodes
        self.evaluation_interval = evaluation_interval
        self.save_interval = save_interval
        
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # í•™ìŠµ í†µê³„
        self.iteration = 0
        self.total_episodes = 0
        self.best_performance = -float('inf')
        self.training_history = []
        
        # ì„±ëŠ¥ ì¶”ì 
        self.discovered_factors = []
        self.performance_history = []
        
        print(f"âœ… AlphaZero Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  MCTS ì‹œë®¬ë ˆì´ì…˜: {mcts_simulations}")
        print(f"  ë°˜ë³µë‹¹ ì—í”¼ì†Œë“œ: {episodes_per_iteration}")
        print(f"  ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {checkpoint_dir}")
    
    def train(self, num_iterations: int):
        """
        AlphaZero í•™ìŠµ ì‹¤í–‰
        
        Args:
            num_iterations: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
        """
        print(f"ğŸš€ AlphaZero í•™ìŠµ ì‹œì‘ ({num_iterations} ë°˜ë³µ)")
        
        for iteration in range(num_iterations):
            self.iteration = iteration
            iteration_start = time.time()
            
            print(f"\n{'='*60}")
            print(f"ë°˜ë³µ {iteration + 1}/{num_iterations}")
            print(f"{'='*60}")
            
            # 1. ìê¸° ëŒ€êµ­ (Self-Play)
            print("ğŸ¯ ìê¸° ëŒ€êµ­ ì¤‘...")
            episode_data = self._self_play()
            
            # 2. ë°ì´í„° ìˆ˜ì§‘
            print("ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            self._collect_training_data(episode_data)
            
            # 3. ì‹ ê²½ë§ í•™ìŠµ
            print("ğŸ§  ì‹ ê²½ë§ í•™ìŠµ ì¤‘...")
            training_stats = self._train_network()
            
            # 4. ì„±ëŠ¥ í‰ê°€
            if (iteration + 1) % self.evaluation_interval == 0:
                print("ğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                eval_stats = self._evaluate_performance()
                print(f"í‰ê°€ ê²°ê³¼: {eval_stats}")
            
            # 5. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (iteration + 1) % self.save_interval == 0:
                self._save_checkpoint()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            iteration_time = time.time() - iteration_start
            self.training_history.append({
                'iteration': iteration + 1,
                'time': iteration_time,
                'training_stats': training_stats,
                'data_samples': self.data_collector.get_stats()
            })
            
            print(f"â±ï¸ ë°˜ë³µ ì™„ë£Œ ì‹œê°„: {iteration_time:.1f}ì´ˆ")
            print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {self.data_collector.get_stats()}")
        
        print(f"\nğŸ‰ AlphaZero í•™ìŠµ ì™„ë£Œ!")
        self._save_final_results()
    
    def _self_play(self) -> List[Dict]:
        """ìê¸° ëŒ€êµ­ ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        
        episode_data = []
        successful_episodes = 0
        
        for episode in range(self.episodes_per_iteration):
            try:
                # ì´ˆê¸° ìƒíƒœì—ì„œ MCTS íƒìƒ‰
                action_probs, root_node = self.mcts.search(
                    root_state=[], 
                    root_need=1
                )
                
                # ì•¡ì…˜ ì„ íƒ (ì˜¨ë„ ì¡°ì ˆ)
                temperature = max(0.1, 1.0 - self.iteration * 0.01)  # ì ì§„ì  ê°ì†Œ
                action = np.random.choice(25, p=action_probs)
                
                # í”„ë¡œê·¸ë¨ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
                program = self._generate_program_from_mcts(root_node, temperature)
                
                if program:
                    # í”„ë¡œê·¸ë¨ í‰ê°€
                    evaluation = self.env.evaluate_program(program)
                    
                    if evaluation['success']:
                        successful_episodes += 1
                        
                        # Factor Poolì— ì €ì¥
                        if self.factor_pool and evaluation['reward'] > 0:
                            self._save_to_factor_pool(program, evaluation)
                    
                    # ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥
                    episode_data.append({
                        'program': program,
                        'evaluation': evaluation,
                        'action_probs': action_probs,
                        'mcts_stats': root_node.tree_stats(max_depth=2)
                    })
                
                if (episode + 1) % 20 == 0:
                    print(f"  ì—í”¼ì†Œë“œ ì§„í–‰: {episode+1}/{self.episodes_per_iteration} "
                          f"(ì„±ê³µ: {successful_episodes})")
                    
            except Exception as e:
                print(f"âš ï¸ ì—í”¼ì†Œë“œ {episode} ì˜¤ë¥˜: {e}")
                continue
        
        self.total_episodes += len(episode_data)
        success_rate = successful_episodes / len(episode_data) if episode_data else 0
        
        print(f"âœ… ìê¸° ëŒ€êµ­ ì™„ë£Œ: {len(episode_data)}ê°œ ì—í”¼ì†Œë“œ, "
              f"ì„±ê³µë¥ : {success_rate:.1%}")
        
        return episode_data
    
    def _generate_program_from_mcts(self, root_node, temperature: float) -> Optional[List[int]]:
        """MCTS íŠ¸ë¦¬ì—ì„œ ì™„ì „í•œ í”„ë¡œê·¸ë¨ ìƒì„±"""
        
        program = []
        node = root_node
        max_steps = 21  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        
        for step in range(max_steps):
            if not node.children:
                break
            
            # ì˜¨ë„ ì¡°ì ˆëœ ì•¡ì…˜ ì„ íƒ
            action_probs = node.get_action_probs(temperature)
            valid_actions = list(node.children.keys())
            
            if not valid_actions:
                break
            
            # ìœ íš¨í•œ ì•¡ì…˜ë“¤ì— ëŒ€í•´ì„œë§Œ í™•ë¥  ì¬ì •ê·œí™”
            valid_probs = np.array([action_probs[a] for a in valid_actions])
            if valid_probs.sum() > 0:
                valid_probs = valid_probs / valid_probs.sum()
                action_idx = np.random.choice(len(valid_actions), p=valid_probs)
                action = valid_actions[action_idx]
            else:
                action = np.random.choice(valid_actions)
            
            program.append(action)
            node = node.children[action]
            
            # í„°ë¯¸ë„ ì²´í¬
            if node.is_terminal:
                break
        
        return program if program else None
    
    def _collect_training_data(self, episode_data: List[Dict]):
        """ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜"""
        
        for episode in episode_data:
            if episode['evaluation']['success']:
                program = episode['program']
                reward = episode['evaluation']['reward']
                
                # í”„ë¡œê·¸ë¨ ìƒì„± ê³¼ì •ì˜ ê° ìƒíƒœì—ì„œ í•™ìŠµ ë°ì´í„° ìƒì„±
                states, policies, values = self.env.generate_training_data(
                    [program], [reward]
                )
                
                if len(states) > 0:
                    self.data_collector.add_episode(states, policies, values, reward)
    
    def _train_network(self) -> Dict:
        """ì‹ ê²½ë§ í•™ìŠµ"""
        
        if self.data_collector.get_stats()['total_samples'] < self.training_batch_size:
            return {'message': 'insufficient_data'}
        
        total_losses = []
        
        for epoch in range(self.training_epochs):
            # ë°°ì¹˜ ìƒ˜í”Œë§
            states, policies, values = self.data_collector.get_batch(self.training_batch_size)
            
            if states is None:
                break
            
            # í•™ìŠµ ìŠ¤í…
            loss_dict = self.trainer.train_step(states, policies, values)
            total_losses.append(loss_dict['total_loss'])
            
            if epoch % 5 == 0:
                print(f"    ì—í¬í¬ {epoch+1}/{self.training_epochs}: "
                      f"ì†ì‹¤ = {loss_dict['total_loss']:.4f}")
        
        avg_loss = np.mean(total_losses) if total_losses else 0
        
        return {
            'epochs': len(total_losses),
            'avg_loss': avg_loss,
            'final_loss': total_losses[-1] if total_losses else 0
        }
    
    def _evaluate_performance(self) -> Dict:
        """í˜„ì¬ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ í‰ê°€"""
        
        evaluation_rewards = []
        successful_evaluations = 0
        
        for _ in range(self.evaluation_episodes):
            try:
                # í‰ê°€ìš© MCTS (ë” ì ì€ ì‹œë®¬ë ˆì´ì…˜)
                eval_mcts = MCTSSearch(
                    network=self.network,
                    num_simulations=200,  # ë¹ ë¥¸ í‰ê°€
                    evaluation_fn=self._evaluate_tokens
                )
                
                action_probs, root_node = eval_mcts.search([], 1)
                program = self._generate_program_from_mcts(root_node, temperature=0.1)
                
                if program:
                    evaluation = self.env.evaluate_program(program)
                    
                    if evaluation['success']:
                        successful_evaluations += 1
                        evaluation_rewards.append(evaluation['reward'])
                    else:
                        evaluation_rewards.append(-1.0)
            
            except Exception as e:
                evaluation_rewards.append(-1.0)
        
        avg_reward = np.mean(evaluation_rewards) if evaluation_rewards else -1.0
        success_rate = successful_evaluations / self.evaluation_episodes
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        if avg_reward > self.best_performance:
            self.best_performance = avg_reward
            self._save_best_model()
        
        eval_stats = {
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'episodes': len(evaluation_rewards),
            'best_performance': self.best_performance
        }
        
        self.performance_history.append(eval_stats)
        
        return eval_stats
    
    def _evaluate_tokens(self, tokens: List[int]) -> float:
        """í† í° ì‹œí€€ìŠ¤ í‰ê°€ (MCTSìš©)"""
        try:
            result = self.env.evaluate_program(tokens)
            return result['reward'] if result['success'] else -1.0
        except:
            return -1.0
    
    def _save_to_factor_pool(self, tokens: List[int], evaluation: Dict):
        """ìš°ìˆ˜í•œ í”„ë¡œê·¸ë¨ì„ Factor Poolì— ì €ì¥"""
        
        if not self.factor_pool:
            return
        
        try:
            # ê°€ìƒì˜ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” evaluationì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
            reward = evaluation['reward']
            info = evaluation.get('info', {})
            
            dates = pd.date_range('2024-01-01', periods=1000, freq='H')
            pnl = pd.Series(np.random.normal(reward/1000, 0.01, 1000), index=dates)
            equity = (1 + pnl).cumprod()
            signal = pd.Series(np.random.normal(0, 1, 1000), index=dates)
            
            reward_info = {
                'total_reward': reward,
                'components': info.get('reward_components', {}),
                'future_leak': False,
                'validation': {'mcts_evaluation': True}
            }
            
            factor_id = self.factor_pool.add_factor(
                tokens=tokens,
                formula=evaluation.get('formula', f"MCTS_Program_{len(tokens)}"),
                pnl=pnl,
                equity=equity,
                signal=signal,
                reward_info=reward_info,
                model_version=f"MCTS_v{self.iteration}",
                training_episode=self.total_episodes
            )
            
            self.discovered_factors.append({
                'factor_id': factor_id,
                'tokens': tokens,
                'reward': reward,
                'iteration': self.iteration
            })
            
        except Exception as e:
            print(f"âš ï¸ Factor Pool ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        
        checkpoint_path = self.checkpoint_dir / f"checkpoint_iter_{self.iteration + 1}.pt"
        self.trainer.save_model(str(checkpoint_path))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'iteration': self.iteration + 1,
            'total_episodes': self.total_episodes,
            'best_performance': self.best_performance,
            'training_history': self.training_history,
            'performance_history': self.performance_history,
            'discovered_factors': len(self.discovered_factors)
        }
        
        metadata_path = self.checkpoint_dir / f"metadata_iter_{self.iteration + 1}.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def _save_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        best_path = self.checkpoint_dir / "best_model.pt"
        self.trainer.save_model(str(best_path))
        print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {self.best_performance:.4f}")
    
    def _save_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_path = self.checkpoint_dir / "final_model.pt"
        self.trainer.save_model(str(final_path))
        
        # ë°œê²¬ëœ íŒ©í„°ë“¤ ì €ì¥
        if self.discovered_factors:
            factors_path = self.checkpoint_dir / "discovered_factors.json"
            with open(factors_path, 'w') as f:
                json.dump(self.discovered_factors, f, indent=2)
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        stats_path = self.checkpoint_dir / "training_stats.json"
        final_stats = {
            'total_iterations': self.iteration + 1,
            'total_episodes': self.total_episodes,
            'best_performance': self.best_performance,
            'final_performance': self.performance_history[-1] if self.performance_history else None,
            'discovered_factors_count': len(self.discovered_factors),
            'training_history': self.training_history,
            'performance_history': self.performance_history
        }
        
        with open(stats_path, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.checkpoint_dir}")
        print(f"ğŸ“Š ì´ ë°œê²¬ëœ íŒ©í„°: {len(self.discovered_factors)}ê°œ")
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {self.best_performance:.4f}")


# í…ŒìŠ¤íŠ¸ ë° ì‹œì—°
if __name__ == "__main__":
    print("ğŸ¤– AlphaZero Trainer í…ŒìŠ¤íŠ¸")
    
    # ë”ë¯¸ ë°ì´í„°ì™€ í™˜ê²½ ì„¤ì •
    from ..data import ParquetCache
    from ..pool import FactorPool
    
    # ë”ë¯¸ ë°ì´í„°
    dates = pd.date_range('2024-01-01', periods=1000, freq='H')
    df = pd.DataFrame({
        'open': np.random.randn(1000).cumsum() + 100,
        'high': np.random.randn(1000).cumsum() + 102,
        'low': np.random.randn(1000).cumsum() + 98,
        'close': np.random.randn(1000).cumsum() + 100,
        'volume': np.random.randint(1000, 10000, 1000)
    }, index=dates)
    
    # í™˜ê²½ê³¼ ë„¤íŠ¸ì›Œí¬ ìƒì„±
    config = RLCConfig()
    env = MCTSFactorEnv(df, config)
    network = PolicyValueNetwork()
    factor_pool = FactorPool("test_mcts_pool")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± (í…ŒìŠ¤íŠ¸ìš© ê°„ì†Œí™” ì„¤ì •)
    trainer = AlphaZeroTrainer(
        env=env,
        network=network,
        factor_pool=factor_pool,
        episodes_per_iteration=5,  # í…ŒìŠ¤íŠ¸ìš© ì ì€ ìˆ˜
        mcts_simulations=50,       # í…ŒìŠ¤íŠ¸ìš© ì ì€ ìˆ˜
        training_epochs=2,         # í…ŒìŠ¤íŠ¸ìš© ì ì€ ìˆ˜
        checkpoint_dir="test_mcts_checkpoints"
    )
    
    # ì§§ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸
    print("ì§§ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    trainer.train(num_iterations=2)
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
