#!/usr/bin/env python3
"""
MCTS íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜

AlphaZero ìŠ¤íƒ€ì¼ Monte Carlo Tree Search:
1. Selection: UCB1 + ì •ì±… í™•ë¥ ë¡œ ë¦¬í”„ ë…¸ë“œê¹Œì§€ íƒìƒ‰
2. Expansion: ì‹ ê²½ë§ìœ¼ë¡œ ì •ì±…ê³¼ ê°€ì¹˜ ì˜ˆì¸¡í•˜ì—¬ ë…¸ë“œ í™•ì¥
3. Evaluation: í„°ë¯¸ë„ ë…¸ë“œë©´ ì‹¤ì œ í‰ê°€, ì•„ë‹ˆë©´ ì‹ ê²½ë§ ê°€ì¹˜ ì‚¬ìš©
4. Backup: ë¦¬í”„ì—ì„œ ë£¨íŠ¸ê¹Œì§€ ê°€ì¹˜ ì •ë³´ ë°±í”„ë¡œíŒŒê²Œì´ì…˜
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Callable
import time
import math

from .mcts_node import MCTSNode, MCTSStats
from .neural_network import PolicyValueNetwork


class MCTSSearch:
    """Monte Carlo Tree Search íƒìƒ‰ ì—”ì§„"""
    
    def __init__(
        self,
        network: PolicyValueNetwork,
        c_puct: float = 1.0,
        num_simulations: int = 800,
        evaluation_fn: Optional[Callable] = None,
        add_noise: bool = True,
        noise_alpha: float = 0.3,
        noise_epsilon: float = 0.25
    ):
        """
        Args:
            network: ì •ì±…-ê°€ì¹˜ ì‹ ê²½ë§
            c_puct: UCB íƒìƒ‰ ìƒìˆ˜
            num_simulations: ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜
            evaluation_fn: í„°ë¯¸ë„ ë…¸ë“œ í‰ê°€ í•¨ìˆ˜
            add_noise: ë£¨íŠ¸ì—ì„œ ë””ë¦¬í´ë ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ ì—¬ë¶€
            noise_alpha: ë””ë¦¬í´ë ˆ ë…¸ì´ì¦ˆ ì•ŒíŒŒ íŒŒë¼ë¯¸í„°
            noise_epsilon: ë…¸ì´ì¦ˆ í˜¼í•© ë¹„ìœ¨
        """
        self.network = network
        self.c_puct = c_puct
        self.num_simulations = num_simulations
        self.evaluation_fn = evaluation_fn
        self.add_noise = add_noise
        self.noise_alpha = noise_alpha
        self.noise_epsilon = noise_epsilon
        
        # í†µê³„
        self.stats = MCTSStats()
        
        # ìºì‹œ (ë™ì¼í•œ ìƒíƒœì— ëŒ€í•œ í‰ê°€ ê²°ê³¼)
        self.evaluation_cache: Dict[tuple, float] = {}
        
        print(f"âœ… MCTS Search ì´ˆê¸°í™” (simulations: {num_simulations}, c_puct: {c_puct})")
    
    def search(self, root_state: List[int], root_need: int) -> Tuple[np.ndarray, MCTSNode]:
        """
        MCTS íƒìƒ‰ ì‹¤í–‰
        
        Args:
            root_state: ë£¨íŠ¸ ìƒíƒœ (í† í° ì‹œí€€ìŠ¤)
            root_need: ë£¨íŠ¸ì—ì„œ í•„ìš”í•œ í† í° ìˆ˜
        
        Returns:
            action_probs: ì•¡ì…˜ í™•ë¥  ë¶„í¬ [25]
            root_node: íƒìƒ‰ì´ ì™„ë£Œëœ ë£¨íŠ¸ ë…¸ë“œ
        """
        # ë£¨íŠ¸ ë…¸ë“œ ìƒì„±
        root = MCTSNode(tokens=root_state, need=root_need)
        
        # ë£¨íŠ¸ ë…¸ë“œ í™•ì¥
        if not root.is_terminal:
            obs = self._state_to_observation(root_state, root_need)
            policy_probs, value = self.network.predict(obs)
            
            # ë””ë¦¬í´ë ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ (íƒìƒ‰ ë‹¤ì–‘ì„± ì¦ì§„)
            if self.add_noise:
                noise = np.random.dirichlet([self.noise_alpha] * 25)
                policy_probs = (1 - self.noise_epsilon) * policy_probs + self.noise_epsilon * noise
            
            root.expand(policy_probs, value)
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats.reset()
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        for i in range(self.num_simulations):
            self._simulate(root)
            
            if (i + 1) % 100 == 0:
                print(f"  ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰: {i+1}/{self.num_simulations}")
        
        # ì•¡ì…˜ í™•ë¥  ë¶„í¬ ê³„ì‚°
        action_probs = root.get_action_probs(temperature=1.0)
        
        print(f"âœ… MCTS íƒìƒ‰ ì™„ë£Œ: {self.stats}")
        
        return action_probs, root
    
    def _simulate(self, root: MCTSNode):
        """ë‹¨ì¼ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
        
        path = []
        node = root
        
        # 1. Selection: ë¦¬í”„ ë…¸ë“œê¹Œì§€ íƒìƒ‰
        while node.is_expanded and not node.is_terminal:
            node = node.select_child(self.c_puct)
            path.append(node)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats.total_simulations += 1
        self.stats.average_depth = (self.stats.average_depth * (self.stats.total_simulations - 1) + len(path)) / self.stats.total_simulations
        self.stats.max_depth = max(self.stats.max_depth, len(path))
        
        # 2. Evaluation
        if node.is_terminal:
            # í„°ë¯¸ë„ ë…¸ë“œ: ì‹¤ì œ í‰ê°€
            value = self._evaluate_terminal(node)
            self.stats.terminal_reaches += 1
        else:
            # 3. Expansion: ë¹„í„°ë¯¸ë„ ë…¸ë“œ í™•ì¥ í›„ í‰ê°€
            obs = self._state_to_observation(node.tokens, node.need)
            policy_probs, value = self.network.predict(obs)
            node.expand(policy_probs, value)
            self.stats.total_expansions += 1
        
        # 4. Backup: ê°€ì¹˜ ì •ë³´ ë°±í”„ë¡œíŒŒê²Œì´ì…˜
        node.backup(value)
    
    def _evaluate_terminal(self, node: MCTSNode) -> float:
        """í„°ë¯¸ë„ ë…¸ë“œ í‰ê°€"""
        
        if self.evaluation_fn is None:
            # ê¸°ë³¸ í‰ê°€: í”„ë¡œê·¸ë¨ ê¸¸ì´ í˜ë„í‹°
            return -len(node.tokens) * 0.01
        
        # ìºì‹œ í™•ì¸
        cache_key = tuple(node.tokens)
        if cache_key in self.evaluation_cache:
            self.stats.cache_hits += 1
            return self.evaluation_cache[cache_key]
        
        # ì‹¤ì œ í‰ê°€ (ë°±í…ŒìŠ¤íŠ¸)
        try:
            reward = self.evaluation_fn(node.tokens)
            self.evaluation_cache[cache_key] = reward
            return reward
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì˜¤ë¥˜: {e}")
            return -1.0
    
    def _state_to_observation(self, tokens: List[int], need: int) -> np.ndarray:
        """ìƒíƒœë¥¼ ê´€ì¸¡ ë²¡í„°ë¡œ ë³€í™˜ (ê¸°ì¡´ í™˜ê²½ê³¼ ë™ì¼í•œ í˜•ì‹)"""
        
        # ê¸°ì¡´ ProgramEnv._obs() ë¡œì§ ì‚¬ìš©
        obs = np.zeros(23, dtype=np.float32)
        
        # í† í° íˆìŠ¤í† ê·¸ë¨ (0~24 â†’ ì²« 25ì°¨ì› ì¤‘ 23ê°œ ì‚¬ìš©)
        if tokens:
            for tok in tokens:
                if 0 <= tok < 23:  # 23ì°¨ì› ì œí•œ
                    obs[tok] += 1
        
        # ì •ê·œí™” (ìµœëŒ€ ê¸¸ì´ 21 ê¸°ì¤€)
        obs = obs / 21.0
        
        return obs
    
    def get_best_action(self, root: MCTSNode) -> int:
        """ê°€ì¥ ì¢‹ì€ ì•¡ì…˜ ì„ íƒ (ë°©ë¬¸ íšŸìˆ˜ ê¸°ì¤€)"""
        if not root.children:
            return 0  # ê¸°ë³¸ê°’
        
        return max(root.children.keys(), 
                  key=lambda a: root.children[a].visit_count)
    
    def get_principal_variation(self, root: MCTSNode, max_depth: int = 10) -> List[int]:
        """
        ì£¼ìš” ë³€í™” (ê°€ì¥ ì¢‹ì€ ê²½ë¡œ) ë°˜í™˜
        
        Args:
            root: ë£¨íŠ¸ ë…¸ë“œ
            max_depth: ìµœëŒ€ ê¹Šì´
        
        Returns:
            ìµœê³  ë°©ë¬¸ íšŸìˆ˜ ê²½ë¡œì˜ ì•¡ì…˜ ì‹œí€€ìŠ¤
        """
        pv = []
        node = root
        
        for _ in range(max_depth):
            if not node.children:
                break
            
            best_child = node.get_best_child()
            if best_child is None:
                break
            
            pv.append(best_child.action)
            node = best_child
        
        return pv
    
    def clear_cache(self):
        """í‰ê°€ ìºì‹œ ì´ˆê¸°í™”"""
        self.evaluation_cache.clear()
        print("ğŸ—‘ï¸ MCTS ìºì‹œ ì´ˆê¸°í™”ë¨")


class AdaptiveMCTS(MCTSSearch):
    """ì ì‘ì  MCTS (ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆ)"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.min_simulations = 200
        self.max_simulations = 1600
        self.confidence_threshold = 0.1
    
    def search(self, root_state: List[int], root_need: int) -> Tuple[np.ndarray, MCTSNode]:
        """ì ì‘ì  íƒìƒ‰: í™•ì‹ ì´ ì¶©ë¶„í•  ë•Œê¹Œì§€ ì‹œë®¬ë ˆì´ì…˜"""
        
        root = MCTSNode(tokens=root_state, need=root_need)
        
        if not root.is_terminal:
            obs = self._state_to_observation(root_state, root_need)
            policy_probs, value = self.network.predict(obs)
            root.expand(policy_probs, value)
        
        # ìµœì†Œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        for i in range(self.min_simulations):
            self._simulate(root)
        
        # í™•ì‹ ë„ ê¸°ë°˜ ì¶”ê°€ ì‹œë®¬ë ˆì´ì…˜
        for i in range(self.min_simulations, self.max_simulations, 50):
            # í˜„ì¬ ìµœê³  ì•¡ì…˜ì˜ í™•ì‹ ë„ ê³„ì‚°
            if root.children:
                visit_counts = [child.visit_count for child in root.children.values()]
                total_visits = sum(visit_counts)
                
                if total_visits > 0:
                    max_visits = max(visit_counts)
                    confidence = max_visits / total_visits
                    
                    # ì¶©ë¶„íˆ í™•ì‹ í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
                    if confidence > (1 - self.confidence_threshold):
                        print(f"  ì¡°ê¸° ì¢…ë£Œ (í™•ì‹ ë„: {confidence:.3f}, ì‹œë®¬ë ˆì´ì…˜: {i})")
                        break
            
            # ì¶”ê°€ ì‹œë®¬ë ˆì´ì…˜
            for _ in range(50):
                self._simulate(root)
        
        action_probs = root.get_action_probs(temperature=1.0)
        return action_probs, root


# í…ŒìŠ¤íŠ¸ ë° ì‹œì—°
if __name__ == "__main__":
    print("ğŸ” MCTS Search í…ŒìŠ¤íŠ¸")
    
    from .neural_network import PolicyValueNetwork
    
    # ë„¤íŠ¸ì›Œí¬ì™€ MCTS ìƒì„±
    network = PolicyValueNetwork()
    mcts = MCTSSearch(network, num_simulations=100)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ íšŸìˆ˜
    
    # ë”ë¯¸ í‰ê°€ í•¨ìˆ˜
    def dummy_eval(tokens):
        return np.random.uniform(-1, 1)
    
    mcts.evaluation_fn = dummy_eval
    
    # íƒìƒ‰ í…ŒìŠ¤íŠ¸
    print("íƒìƒ‰ í…ŒìŠ¤íŠ¸...")
    root_state = [1, 2]  # ì´ˆê¸° í† í°
    root_need = 2        # 2ê°œ ë” í•„ìš”
    
    action_probs, root_node = mcts.search(root_state, root_need)
    
    print(f"ì•¡ì…˜ í™•ë¥  ë¶„í¬: {action_probs}")
    print(f"ìµœê³  ì•¡ì…˜: {mcts.get_best_action(root_node)}")
    print(f"ì£¼ìš” ë³€í™”: {mcts.get_principal_variation(root_node, 5)}")
    print(f"ë£¨íŠ¸ ë…¸ë“œ ì •ë³´:\n{root_node}")
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
