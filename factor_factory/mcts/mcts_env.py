#!/usr/bin/env python3
"""
MCTSìš© Factor Environment

ê¸°ì¡´ ProgramEnvë¥¼ MCTSì— ë§ê²Œ ì¡°ì •:
- ìŠ¤í…ë³„ ì‹¤í–‰ ëŒ€ì‹  ì™„ì„±ëœ í”„ë¡œê·¸ë¨ì˜ ì¼ê´„ í‰ê°€
- ì‹ ê²½ë§ í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘
- ê¸°ì¡´ PPO í™˜ê²½ê³¼ í˜¸í™˜ë˜ëŠ” í‰ê°€ ë°©ì‹
"""

import numpy as np
import pandas as pd
import time
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

from ..rlc.env import ProgramEnv, RLCConfig
from ..rlc.grammar import ARITY
from ..rlc.utils import tokens_to_infix, calc_tree_depth
from ..rlc.compiler import eval_prefix
from ..rlc.signal_generator import generate_signal_realtime
from ..backtest.realistic_engine import realistic_backtest
from ..backtest.fast_engine import get_fast_backtest_engine
from ..rlc.enhanced_cache import get_fast_program_cache
from ..data import ParquetCache


class MCTSFactorEnv:
    """MCTSìš© íŒ©í„° í™˜ê²½"""
    
    def __init__(self, df: pd.DataFrame, config: RLCConfig):
        """
        Args:
            df: ê°€ê²© ë°ì´í„°
            config: RL í™˜ê²½ ì„¤ì •
        """
        self.df = df
        self.cfg = config
        
        # ê¸°ì¡´ í™˜ê²½ì˜ ìºì‹œì™€ í†µê³„ ì‹œìŠ¤í…œ í™œìš©
        self.base_env = ProgramEnv(df, config)
        
        # ê³ ì† ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
        self.backtest_engine = get_fast_backtest_engine(
            commission=config.commission,
            slippage=config.slippage,
            leverage=config.leverage
        )
        
        # í‰ê°€ í†µê³„
        self.total_evaluations = 0
        self.successful_evaluations = 0
        self.evaluation_times = []
        self.cache_hits = 0  # ìºì‹œ íˆíŠ¸ ìˆ˜
        
        print(f"âœ… MCTS Factor Environment ì´ˆê¸°í™”")
        print(f"  ë°ì´í„° í¬ê¸°: {len(df)} í–‰")
        print(f"  ê¸°ê°„: {df.index[0]} ~ {df.index[-1]}")
        print(f"  ê³ ì† ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ í™œì„±í™”")
    
    def evaluate_program(self, tokens: List[int]) -> Dict:
        """
        ì™„ì„±ëœ í”„ë¡œê·¸ë¨ í‰ê°€ - ê³ ì† ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ì‚¬ìš©
        
        Args:
            tokens: ì™„ì„±ëœ í† í° ì‹œí€€ìŠ¤
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        self.total_evaluations += 1
        
        try:
            # 1. ìºì‹œ í™•ì¸
            cache = get_fast_program_cache()
            cached_signal = cache.get(tokens, self.df)
            
            if cached_signal is not None:
                self.cache_hits += 1
                # ìºì‹œëœ ì‹ í˜¸ë¡œ ë¹ ë¥¸ ë°±í…ŒìŠ¤íŠ¸
                result = self._fast_backtest_cached_signal(tokens, cached_signal)
            else:
                # 2. ì‹ í˜¸ ìƒì„± ë° ë°±í…ŒìŠ¤íŠ¸
                result = self._evaluate_with_fast_engine(tokens)
            
            # í‰ê°€ ì‹œê°„ ê¸°ë¡
            eval_time = time.time() - start_time
            self.evaluation_times.append(eval_time)
            
            if result['success']:
                self.successful_evaluations += 1
            
            return result
            
        except Exception as e:
            eval_time = time.time() - start_time
            self.evaluation_times.append(eval_time)
            
            return {
                'success': False,
                'reward': -1.0,
                'error': str(e),
                'tokens': tokens,
                'eval_time': eval_time
            }
    
    def _evaluate_with_fast_engine(self, tokens: List[int]) -> Dict:
        """ê³ ì† ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ì„ ì‚¬ìš©í•œ í‰ê°€"""
        
        try:
            # 1. ì‹ í˜¸ ìƒì„± (í–¥ìƒëœ ìºì‹œ ì‚¬ìš©)
            signal = eval_prefix(tokens, self.df, use_fast_cache=True)
            
            # 2. ê¸°ë³¸ ê²€ì¦
            if signal.isna().all() or not signal.std() > 0:
                return {
                    'success': False,
                    'reward': -1.0,
                    'error': 'Invalid signal (all NaN or zero variance)',
                    'tokens': tokens
                }
            
            # 3. ê³ ì† ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            price = self.df['close']
            backtest_result = self.backtest_engine.backtest(price, signal)
            
            if not backtest_result['success']:
                return {
                    'success': False,
                    'reward': -1.0,
                    'error': 'Backtest failed',
                    'tokens': tokens
                }
            
            # 4. ë³µì¡ì„± í˜ë„í‹° ì ìš©
            complexity_penalty = self._calculate_complexity_penalty(tokens)
            base_reward = backtest_result['metrics']['sharpe']
            final_reward = base_reward - complexity_penalty
            
            # 5. ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            return {
                'success': True,
                'reward': float(final_reward),
                'base_reward': float(base_reward),
                'complexity_penalty': float(complexity_penalty),
                'metrics': backtest_result['metrics'],
                'equity': backtest_result['equity'],
                'pnl': backtest_result['pnl'],
                'signal': signal,
                'formula': tokens_to_infix(tokens),
                'tokens': tokens,
                'depth': calc_tree_depth(tokens),
                'length': len(tokens)
            }
            
        except Exception as e:
            return {
                'success': False,
                'reward': -1.0,
                'error': f'Evaluation error: {str(e)}',
                'tokens': tokens
            }
    
    def _fast_backtest_cached_signal(self, tokens: List[int], signal: pd.Series) -> Dict:
        """ìºì‹œëœ ì‹ í˜¸ë¡œ ë¹ ë¥¸ ë°±í…ŒìŠ¤íŠ¸"""
        
        try:
            # ê³ ì† ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            price = self.df['close']
            backtest_result = self.backtest_engine.backtest(price, signal)
            
            if not backtest_result['success']:
                return {
                    'success': False,
                    'reward': -1.0,
                    'error': 'Cached backtest failed',
                    'tokens': tokens
                }
            
            # ë³µì¡ì„± í˜ë„í‹° ì ìš©
            complexity_penalty = self._calculate_complexity_penalty(tokens)
            base_reward = backtest_result['metrics']['sharpe']
            final_reward = base_reward - complexity_penalty
            
            return {
                'success': True,
                'reward': float(final_reward),
                'base_reward': float(base_reward),
                'complexity_penalty': float(complexity_penalty),
                'metrics': backtest_result['metrics'],
                'equity': backtest_result['equity'],
                'pnl': backtest_result['pnl'],
                'signal': signal,
                'formula': tokens_to_infix(tokens),
                'tokens': tokens,
                'depth': calc_tree_depth(tokens),
                'length': len(tokens),
                'cached': True  # ìºì‹œ ì‚¬ìš© í‘œì‹œ
            }
            
        except Exception as e:
            return {
                'success': False,
                'reward': -1.0,
                'error': f'Cached evaluation error: {str(e)}',
                'tokens': tokens
            }
    
    def _evaluate_with_base_env(self, tokens: List[int]) -> Dict:
        """ê¸°ì¡´ í™˜ê²½ì„ ì‚¬ìš©í•œ í‰ê°€"""
        
        # í™˜ê²½ ë¦¬ì…‹
        self.base_env.reset()
        
        # í† í° ì‹œí€€ìŠ¤ ë‹¨ê³„ë³„ ì‹¤í–‰
        reward = 0.0
        info = {}
        
        for i, token in enumerate(tokens):
            obs, step_reward, done, truncated, step_info = self.base_env.step(token)
            reward += step_reward
            info.update(step_info)
            
            if done:
                break
        
        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        success = (
            done and 
            not info.get('invalid', False) and
            not info.get('error', False) and
            not info.get('future_leak', False)
        )
        
        # ë³µì¡ì„± í˜ë„í‹° ì¶”ê°€
        if success:
            complexity_penalty = self._calculate_complexity_penalty(tokens)
            reward = float(reward) - complexity_penalty
        
        return {
            'success': success,
            'reward': float(reward),
            'info': info,
            'tokens': tokens,
            'formula': tokens_to_infix(tokens) if success else None
        }
    
    def get_legal_actions(self, tokens: List[int], need: int) -> List[int]:
        """í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ ì•¡ì…˜ë“¤"""
        
        legal_actions = []
        max_len = min(self.cfg.max_len, 20)  # ìµœëŒ€ ê¹Šì´ 20ìœ¼ë¡œ í™•ì¥
        
        for token in range(25):  # 0~24
            # ê¸¸ì´ ì œí•œ
            if len(tokens) >= max_len:
                continue
            
            # ìƒˆë¡œìš´ need ê³„ì‚°
            new_need = need - 1 + ARITY[token]
            
            # need ì¡°ê±´ í™•ì¸
            if new_need > max_len - len(tokens) - 1:
                continue
            if new_need < 0:
                continue
            
            # LAG1 í† í°(12) ê³¼ë‹¤ ì‚¬ìš© ë°©ì§€ (4ê°œê¹Œì§€ í—ˆìš©)
            if token == 12:  # LAG1
                lag_count = tokens.count(12)
                if lag_count >= 4:  # 4ê°œ ì´ìƒ ì‚¬ìš©í–ˆìœ¼ë©´ ì œì™¸
                    continue
            
            # ì—°ì† ê°™ì€ í† í° ì‚¬ìš© ë°©ì§€
            if len(tokens) >= 2 and tokens[-1] == tokens[-2] == token:
                continue
            
            legal_actions.append(token)
        
        return legal_actions
    
    def _calculate_complexity_penalty(self, tokens: List[int]) -> float:
        """ë³µì¡ì„± í˜ë„í‹° ê³„ì‚°"""
        
        # ê¸°ë³¸ ê¸¸ì´ í˜ë„í‹° (ë§¤ìš° ê°€ë²¼ì›€)
        length_penalty = len(tokens) * 0.005
        
        # ë°˜ë³µ íŒ¨í„´ í˜ë„í‹° (ê°™ì€ í† í° ì—°ì† ì‚¬ìš©)
        repetition_penalty = 0.0
        consecutive_count = 1
        max_consecutive = 1
        
        for i in range(1, len(tokens)):
            if tokens[i] == tokens[i-1]:
                consecutive_count += 1
                max_consecutive = max(max_consecutive, consecutive_count)
            else:
                consecutive_count = 1
        
        # 3ë²ˆ ì´ìƒ ì—°ì† ì‚¬ìš©ì‹œ í˜ë„í‹°
        if max_consecutive >= 3:
            repetition_penalty = (max_consecutive - 2) * 0.2
        
        # LAG ê³¼ë‹¤ ì‚¬ìš© í˜ë„í‹° (í† í° 12ê°€ LAG1)
        lag_count = tokens.count(12)  # LAG1 í† í°
        lag_penalty = max(0, (lag_count - 4) * 0.08)  # 4ê°œ ì´ˆê³¼ì‹œ ê°€ë²¼ìš´ í˜ë„í‹°
        
        # ê¹Šì´ í˜ë„í‹° (18 ì´ìƒì‹œë§Œ ì ìš©, ë§¤ìš° ê°€ë²¼ì›€)
        depth_penalty = max(0, (len(tokens) - 18) * 0.02)
        
        total_penalty = length_penalty + repetition_penalty + lag_penalty + depth_penalty
        
        return total_penalty
    
    def is_terminal(self, tokens: List[int], need: int) -> bool:
        """í„°ë¯¸ë„ ìƒíƒœ ì—¬ë¶€"""
        return need == 0
    
    def state_to_observation(self, tokens: List[int], need: int) -> np.ndarray:
        """ìƒíƒœë¥¼ ì‹ ê²½ë§ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        obs = np.zeros(23, dtype=np.float32)
        
        # í† í° íˆìŠ¤í† ê·¸ë¨
        if tokens:
            for tok in tokens:
                if 0 <= tok < 23:
                    obs[tok] += 1
        
        # ì •ê·œí™”
        obs = obs / max(1, len(tokens))
        
        return obs
    
    def generate_training_data(
        self, 
        action_sequences: List[List[int]], 
        rewards: List[float]
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        MCTS íƒìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì‹ ê²½ë§ í•™ìŠµ ë°ì´í„° ìƒì„±
        
        Args:
            action_sequences: ì•¡ì…˜ ì‹œí€€ìŠ¤ë“¤
            rewards: ê° ì‹œí€€ìŠ¤ì˜ ë³´ìƒ
        
        Returns:
            states: ìƒíƒœ ë°°ì¹˜ [N, obs_dim]
            policy_targets: ì •ì±… íƒ€ê²Ÿ [N, action_dim] 
            value_targets: ê°€ì¹˜ íƒ€ê²Ÿ [N]
        """
        states = []
        policy_targets = []
        value_targets = []
        
        for seq, reward in zip(action_sequences, rewards):
            # ì‹œí€€ìŠ¤ì˜ ê° ì¤‘ê°„ ìƒíƒœì—ì„œ í•™ìŠµ ë°ì´í„° ìƒì„±
            need = 1
            
            for i, action in enumerate(seq):
                current_tokens = seq[:i]
                
                # ìƒíƒœ í‘œí˜„
                state = self.state_to_observation(current_tokens, need)
                states.append(state)
                
                # ì •ì±… íƒ€ê²Ÿ (ì‹¤ì œ ì„ íƒëœ ì•¡ì…˜ì— í™•ë¥  1)
                policy_target = np.zeros(25)
                policy_target[action] = 1.0
                policy_targets.append(policy_target)
                
                # ê°€ì¹˜ íƒ€ê²Ÿ (ìµœì¢… ë³´ìƒ)
                value_targets.append(reward)
                
                # need ì—…ë°ì´íŠ¸
                need = need - 1 + ARITY[action]
                
                if need == 0:
                    break
        
        return (
            np.array(states, dtype=np.float32),
            np.array(policy_targets, dtype=np.float32),
            np.array(value_targets, dtype=np.float32)
        )
    
    def get_statistics(self) -> Dict:
        """í™˜ê²½ í†µê³„ ë°˜í™˜ - í–¥ìƒëœ ìºì‹œ ì •ë³´ í¬í•¨"""
        
        success_rate = (
            self.successful_evaluations / self.total_evaluations 
            if self.total_evaluations > 0 else 0
        )
        
        avg_eval_time = (
            np.mean(self.evaluation_times) 
            if self.evaluation_times else 0
        )
        
        cache_hit_rate = (
            self.cache_hits / self.total_evaluations
            if self.total_evaluations > 0 else 0
        )
        
        # ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ í†µê³„
        backtest_stats = self.backtest_engine.get_stats()
        
        # í–¥ìƒëœ ìºì‹œ í†µê³„
        cache = get_fast_program_cache()
        cache_stats = cache.get_stats()
        
        base_stats = {}
        if hasattr(self.base_env, 'total_programs_evaluated'):
            base_stats = {
                'base_cache_hits': getattr(self.base_env, 'cache_hits', 0),
                'base_total_programs': getattr(self.base_env, 'total_programs_evaluated', 0),
                'validation_failures': getattr(self.base_env, 'validation_failures', 0),
                'total_validations': getattr(self.base_env, 'total_validations', 0)
            }
        
        return {
            'total_evaluations': self.total_evaluations,
            'successful_evaluations': self.successful_evaluations,
            'success_rate': success_rate,
            'avg_eval_time': avg_eval_time,
            'cache_hits': self.cache_hits,
            'cache_hit_rate': cache_hit_rate,
            'min_eval_time': min(self.evaluation_times) if self.evaluation_times else 0,
            'max_eval_time': max(self.evaluation_times) if self.evaluation_times else 0,
            'total_eval_time': sum(self.evaluation_times),
            'backtest_engine_stats': backtest_stats,
            'enhanced_cache_stats': cache_stats,
            **base_stats
        }
    
    def reset_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.total_evaluations = 0
        self.successful_evaluations = 0
        self.evaluation_times = []
        self.cache_hits = 0
        
        # ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ í†µê³„ ì´ˆê¸°í™”
        if hasattr(self.backtest_engine, 'stats'):
            for key in self.backtest_engine.stats:
                if isinstance(self.backtest_engine.stats[key], (int, float)):
                    self.backtest_engine.stats[key] = 0
        
        # ê¸°ì¡´ í™˜ê²½ í†µê³„ë„ ì´ˆê¸°í™”
        if hasattr(self.base_env, 'reset_statistics'):
            self.base_env.reset_statistics()
        if hasattr(self.base_env, 'cache_hits'):
            self.base_env.cache_hits = 0
            self.base_env.total_programs_evaluated = 0
            self.base_env.validation_failures = 0
            self.base_env.total_validations = 0


class MCTSDataCollector:
    """MCTS í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, max_samples: int = 10000):
        self.max_samples = max_samples
        self.states = []
        self.policy_targets = []
        self.value_targets = []
        self.rewards = []
        
    def add_episode(
        self, 
        states: np.ndarray, 
        policies: np.ndarray, 
        values: np.ndarray, 
        final_reward: float
    ):
        """ì—í”¼ì†Œë“œ ë°ì´í„° ì¶”ê°€"""
        
        # ê°€ì¹˜ íƒ€ê²Ÿì„ ìµœì¢… ë³´ìƒìœ¼ë¡œ ì„¤ì • (ë‹¨ìˆœí™”)
        episode_values = np.full(len(states), final_reward)
        
        self.states.extend(states)
        self.policy_targets.extend(policies)
        self.value_targets.extend(episode_values)
        self.rewards.append(final_reward)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        if len(self.states) > self.max_samples:
            excess = len(self.states) - self.max_samples
            self.states = self.states[excess:]
            self.policy_targets = self.policy_targets[excess:]
            self.value_targets = self.value_targets[excess:]
    
    def get_batch(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ëœë¤ ë°°ì¹˜ ìƒ˜í”Œë§"""
        
        if len(self.states) < batch_size:
            return None, None, None
        
        indices = np.random.choice(len(self.states), batch_size, replace=False)
        
        batch_states = np.array([self.states[i] for i in indices])
        batch_policies = np.array([self.policy_targets[i] for i in indices])
        batch_values = np.array([self.value_targets[i] for i in indices])
        
        return batch_states, batch_policies, batch_values
    
    def clear(self):
        """ë°ì´í„° ì´ˆê¸°í™”"""
        self.states.clear()
        self.policy_targets.clear()
        self.value_targets.clear()
        self.rewards.clear()
    
    def get_stats(self) -> Dict:
        """ìˆ˜ì§‘ í†µê³„"""
        return {
            'total_samples': len(self.states),
            'episodes': len(self.rewards),
            'avg_reward': np.mean(self.rewards) if self.rewards else 0,
            'max_reward': np.max(self.rewards) if self.rewards else 0,
            'min_reward': np.min(self.rewards) if self.rewards else 0
        }


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸ­ MCTS Factor Environment í…ŒìŠ¤íŠ¸")
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2024-01-01', periods=1000, freq='H')
    df = pd.DataFrame({
        'open': np.random.randn(1000).cumsum() + 100,
        'high': np.random.randn(1000).cumsum() + 102,
        'low': np.random.randn(1000).cumsum() + 98,
        'close': np.random.randn(1000).cumsum() + 100,
        'volume': np.random.randint(1000, 10000, 1000)
    }, index=dates)
    
    # í™˜ê²½ ìƒì„±
    config = RLCConfig()
    env = MCTSFactorEnv(df, config)
    
    # í”„ë¡œê·¸ë¨ í‰ê°€ í…ŒìŠ¤íŠ¸
    test_tokens = [1, 2, 3, 12, 4]  # ê°„ë‹¨í•œ í”„ë¡œê·¸ë¨
    result = env.evaluate_program(test_tokens)
    
    print(f"í‰ê°€ ê²°ê³¼: {result}")
    print(f"í™˜ê²½ í†µê³„: {env.get_statistics()}")
    
    # ë°ì´í„° ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸
    collector = MCTSDataCollector()
    
    # ë”ë¯¸ ì—í”¼ì†Œë“œ ì¶”ê°€
    states = np.random.randn(5, 23)
    policies = np.random.rand(5, 25)
    values = np.random.randn(5)
    reward = 0.5
    
    collector.add_episode(states, policies, values, reward)
    print(f"ìˆ˜ì§‘ê¸° í†µê³„: {collector.get_stats()}")
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
