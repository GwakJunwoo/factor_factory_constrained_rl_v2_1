#!/usr/bin/env python3
"""
ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

@dataclass
class ImprovedRLCConfig:
    """ê°œì„ ëœ RL í™˜ê²½ ì„¤ì •"""
    max_len: int = 21
    
    # === ê°œì„ ëœ í˜ë„í‹° ê°€ì¤‘ì¹˜ ===
    # ê¸°ì¡´ â†’ ê°œì„ 
    lambda_depth: float = 0.01        # 0.002 â†’ 0.01 (5ë°° ì¦ê°€: ë³µì¡ë„ ê°•ë ¥ ì–µì œ)
    lambda_turnover: float = 0.002    # 0.0005 â†’ 0.002 (4ë°° ì¦ê°€: ê³¼ê±°ë˜ ì–µì œ)
    lambda_const1: float = 0.5        # 2.0 â†’ 0.5 (1/4ë¡œ ê°ì†Œ: ìƒìˆ˜ ì‚¬ìš© í—ˆìš©)
    lambda_std: float = 0.1           # 0.5 â†’ 0.1 (1/5ë¡œ ê°ì†Œ: ë³€ë™ì„± í˜ë„í‹° ì™„í™”)
    
    # === ìƒˆë¡œìš´ í˜ë„í‹° ì¶”ê°€ ===
    lambda_drawdown: float = 2.0      # MDD í˜ë„í‹°
    lambda_consistency: float = 0.5   # ìˆ˜ìµë¥  ì¼ê´€ì„± í˜ë„í‹°
    lambda_skewness: float = 0.1      # í¸í–¥ì„± í˜ë„í‹°
    
    # === ë³´ìƒ ì‹œìŠ¤í…œ íƒ€ì… ===
    reward_type: str = 'improved_basic'  # 'basic', 'improved_basic', 'sharpe', 'multi_objective'
    
    # === ë™ì  ë³´ìƒ ì„¤ì • ===
    adaptive_weights: bool = True
    performance_window: int = 100
    reward_normalization: bool = True
    reward_clipping: float = 3.0

class RewardSystem:
    """ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: ImprovedRLCConfig):
        self.config = config
        self.episode_count = 0
        self.reward_history = []
        self.performance_buffer = []
        
    def calculate_reward(
        self,
        pnl: pd.Series,
        equity: pd.Series,
        signal: pd.Series,
        tokens: List[int],
        depth: int,
        trades: int
    ) -> Dict:
        """í†µí•© ë³´ìƒ ê³„ì‚° í•¨ìˆ˜"""
        
        self.episode_count += 1
        
        if self.config.reward_type == 'basic':
            reward, components = self._basic_reward(pnl, signal, tokens, depth, trades)
        elif self.config.reward_type == 'improved_basic':
            reward, components = self._improved_basic_reward(pnl, equity, signal, tokens, depth, trades)
        elif self.config.reward_type == 'sharpe':
            reward, components = self._sharpe_based_reward(pnl, equity, signal, tokens, depth, trades)
        elif self.config.reward_type == 'multi_objective':
            reward, components = self._multi_objective_reward(pnl, equity, signal, tokens, depth, trades)
        else:
            raise ValueError(f"Unknown reward type: {self.config.reward_type}")
        
        # ì ì‘ì  ê°€ì¤‘ì¹˜ ì ìš©
        if self.config.adaptive_weights:
            reward = self._apply_adaptive_weights(reward, components)
        
        # ë³´ìƒ ì •ê·œí™” ë° í´ë¦¬í•‘
        if self.config.reward_normalization:
            reward = self._normalize_reward(reward)
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.reward_history.append(reward)
        
        return {
            'total_reward': reward,
            'components': components,
            'episode': self.episode_count
        }
    
    def _basic_reward(self, pnl, signal, tokens, depth, trades):
        """ê¸°ì¡´ ë³´ìƒ ì‹œìŠ¤í…œ"""
        pnl_sum = float(pnl.sum())
        
        # ê¸°ì¡´ í˜ë„í‹°
        from collections import Counter
        cnt = Counter(tokens)
        const_ratio = cnt.get(12, 0) / len(tokens)
        std_pen = self.config.lambda_std / (signal.std() + 1e-8)
        
        components = {
            'pnl': pnl_sum,
            'depth_penalty': -self.config.lambda_depth * depth,
            'turnover_penalty': -self.config.lambda_turnover * trades,
            'const_penalty': -self.config.lambda_const1 * const_ratio,
            'std_penalty': -std_pen
        }
        
        total = sum(components.values())
        return total, components
    
    def _improved_basic_reward(self, pnl, equity, signal, tokens, depth, trades):
        """ê°œì„ ëœ ê¸°ë³¸ ë³´ìƒ ì‹œìŠ¤í…œ"""
        pnl_sum = float(pnl.sum())
        
        # ê¸°ì¡´ í˜ë„í‹° (ê°œì„ ëœ ê°€ì¤‘ì¹˜)
        from collections import Counter
        cnt = Counter(tokens)
        const_ratio = cnt.get(12, 0) / len(tokens)
        std_pen = self.config.lambda_std / (signal.std() + 1e-8)
        
        # ìƒˆë¡œìš´ í˜ë„í‹°ë“¤
        max_drawdown = abs((equity / equity.cummax() - 1).min())
        drawdown_penalty = self.config.lambda_drawdown * max_drawdown
        
        # ìˆ˜ìµë¥  ì¼ê´€ì„± (ë³€ë™ ê³„ìˆ˜ì˜ ì—­ìˆ˜)
        pnl_cv = abs(pnl.mean()) / (pnl.std() + 1e-8)
        consistency_bonus = self.config.lambda_consistency * min(pnl_cv, 2.0)
        
        # í¸í–¥ì„± í˜ë„í‹° (ê·¹ë‹¨ì  skewness ì–µì œ)
        try:
            skewness = abs(pnl.skew())
            skewness_penalty = self.config.lambda_skewness * max(0, skewness - 1.0)
        except:
            skewness_penalty = 0.0
        
        components = {
            'pnl': pnl_sum,
            'depth_penalty': -self.config.lambda_depth * depth,
            'turnover_penalty': -self.config.lambda_turnover * trades,
            'const_penalty': -self.config.lambda_const1 * const_ratio,
            'std_penalty': -std_pen,
            'drawdown_penalty': -drawdown_penalty,
            'consistency_bonus': consistency_bonus,
            'skewness_penalty': -skewness_penalty
        }
        
        total = sum(components.values())
        return total, components
    
    def _sharpe_based_reward(self, pnl, equity, signal, tokens, depth, trades):
        """Sharpe Ratio ê¸°ë°˜ ë³´ìƒ"""
        
        # Sharpe Ratio ê³„ì‚°
        sharpe = pnl.mean() / (pnl.std() + 1e-8)
        sharpe_scaled = np.tanh(sharpe * 2.0)  # [-1, 1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
        
        # êµ¬ì¡°ì  í˜ë„í‹° (ê²½ëŸ‰í™”)
        structure_penalty = (
            self.config.lambda_depth * 0.5 * depth +
            self.config.lambda_turnover * 0.5 * trades +
            self.config.lambda_const1 * 0.5 * (tokens.count(12) / len(tokens))
        )
        
        components = {
            'sharpe_ratio': sharpe_scaled * 5.0,  # ì£¼ ë³´ìƒ
            'structure_penalty': -structure_penalty
        }
        
        total = sum(components.values())
        return total, components
    
    def _multi_objective_reward(self, pnl, equity, signal, tokens, depth, trades):
        """ë‹¤ëª©ì  ìµœì í™” ë³´ìƒ"""
        
        # 1. ìˆ˜ìµì„± ì ìˆ˜ (30%)
        total_return = (equity.iloc[-1] - 1) * 100
        return_score = np.tanh(total_return / 10)  # Â±10% ê¸°ì¤€
        
        # 2. ìœ„í—˜ì„± ì ìˆ˜ (25%)
        volatility = pnl.std() * np.sqrt(252 * 24) * 100
        risk_score = max(0, 1 - volatility / 20)  # 20% ë³€ë™ì„± ê¸°ì¤€
        
        # 3. ì•ˆì •ì„± ì ìˆ˜ (25%)
        max_dd = abs((equity / equity.cummax() - 1).min())
        stability_score = max(0, 1 - max_dd / 0.2)  # 20% MDD ê¸°ì¤€
        
        # 4. íš¨ìœ¨ì„± ì ìˆ˜ (20%)
        win_rate = (pnl > 0).sum() / len(pnl)
        efficiency_score = win_rate * 2 - 1  # [0,1] â†’ [-1,1]
        
        # ê°€ì¤‘ í‰ê· 
        multi_score = (
            0.3 * return_score +
            0.25 * risk_score +
            0.25 * stability_score +
            0.2 * efficiency_score
        )
        
        # êµ¬ì¡°ì  í˜ë„í‹° (ê²½ëŸ‰í™”)
        structure_penalty = (
            self.config.lambda_depth * 0.3 * depth +
            self.config.lambda_turnover * 0.3 * trades
        )
        
        components = {
            'multi_objective': multi_score * 3.0,  # ì£¼ ë³´ìƒ
            'return_component': return_score * 0.3 * 3.0,
            'risk_component': risk_score * 0.25 * 3.0,
            'stability_component': stability_score * 0.25 * 3.0,
            'efficiency_component': efficiency_score * 0.2 * 3.0,
            'structure_penalty': -structure_penalty
        }
        
        total = sum(components.values())
        return total, components
    
    def _apply_adaptive_weights(self, reward, components):
        """ì ì‘ì  ê°€ì¤‘ì¹˜ ì ìš©"""
        
        # ì´ˆê¸° ë‹¨ê³„ (0-1000): íƒìƒ‰ ê²©ë ¤
        if self.episode_count <= 1000:
            exploration_bonus = 0.1
            return reward + exploration_bonus
        
        # ì¤‘ê°„ ë‹¨ê³„ (1000-5000): ê· í˜•
        elif self.episode_count <= 5000:
            return reward
        
        # í›„ê¸° ë‹¨ê³„ (5000+): ìˆ˜ë ´ ê²©ë ¤
        else:
            # ì„±ê³¼ê°€ ì •ì²´ë˜ë©´ íƒìƒ‰ ê²©ë ¤
            if len(self.reward_history) >= 100:
                recent_rewards = self.reward_history[-100:]
                if np.std(recent_rewards) < 0.1:  # ì„±ê³¼ ì •ì²´
                    exploration_bonus = 0.05
                    return reward + exploration_bonus
            
            return reward
    
    def _normalize_reward(self, reward):
        """ë³´ìƒ ì •ê·œí™” ë° í´ë¦¬í•‘"""
        
        self.reward_history.append(reward)
        
        # ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ Z-score ì •ê·œí™”
        if len(self.reward_history) > 50:
            history = np.array(self.reward_history[-self.config.performance_window:])
            mean = history.mean()
            std = history.std() + 1e-8
            normalized = (reward - mean) / std
        else:
            normalized = reward
        
        # í´ë¦¬í•‘
        clipped = np.clip(normalized, -self.config.reward_clipping, self.config.reward_clipping)
        
        # íƒ„ì  íŠ¸ ìŠ¤ì¼€ì¼ë§ [-1, 1]
        final_reward = np.tanh(clipped)
        
        return final_reward

# ì‹œì—° í•¨ìˆ˜
def demonstrate_reward_systems():
    """ë‹¤ì–‘í•œ ë³´ìƒ ì‹œìŠ¤í…œ ì‹œì—°"""
    
    print("ğŸ¯ ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œ ì‹œì—°")
    print("=" * 50)
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=1000, freq='H')
    
    # ì‹œë‚˜ë¦¬ì˜¤ 1: ê³ ìˆ˜ìµ ê³ ìœ„í—˜ ì „ëµ
    pnl1 = pd.Series(np.random.normal(0.001, 0.02, 1000), index=dates)
    equity1 = (1 + pnl1).cumprod()
    signal1 = pd.Series(np.random.normal(0, 1, 1000), index=dates)
    
    # ì‹œë‚˜ë¦¬ì˜¤ 2: ì €ìˆ˜ìµ ì €ìœ„í—˜ ì „ëµ  
    pnl2 = pd.Series(np.random.normal(0.0005, 0.005, 1000), index=dates)
    equity2 = (1 + pnl2).cumprod()
    signal2 = pd.Series(np.random.normal(0, 0.5, 1000), index=dates)
    
    # ì‹œë‚˜ë¦¬ì˜¤ 3: ë¶ˆì•ˆì •í•œ ì „ëµ
    pnl3 = pd.Series(np.random.normal(0, 0.03, 1000), index=dates) 
    pnl3.iloc[100:150] = -0.05  # í° ì†ì‹¤ êµ¬ê°„
    equity3 = (1 + pnl3).cumprod()
    signal3 = pd.Series(np.random.normal(0, 2, 1000), index=dates)
    
    scenarios = [
        ("ê³ ìˆ˜ìµ ê³ ìœ„í—˜", pnl1, equity1, signal1, [1,2,3,4,5], 3, 50),
        ("ì €ìˆ˜ìµ ì €ìœ„í—˜", pnl2, equity2, signal2, [1,2,12,12], 2, 10),
        ("ë¶ˆì•ˆì •í•œ ì „ëµ", pnl3, equity3, signal3, [1,2,3,4,5,6,7,8], 5, 100)
    ]
    
    reward_types = ['basic', 'improved_basic', 'sharpe', 'multi_objective']
    
    for reward_type in reward_types:
        print(f"\nğŸ“Š {reward_type.upper()} ë³´ìƒ ì‹œìŠ¤í…œ")
        print("-" * 30)
        
        config = ImprovedRLCConfig(reward_type=reward_type)
        reward_system = RewardSystem(config)
        
        for name, pnl, equity, signal, tokens, depth, trades in scenarios:
            result = reward_system.calculate_reward(pnl, equity, signal, tokens, depth, trades)
            
            print(f"{name:12s}: {result['total_reward']:8.4f}")
            
            # êµ¬ì„± ìš”ì†Œ ì¶œë ¥ (improved_basicë§Œ)
            if reward_type == 'improved_basic':
                for comp, value in result['components'].items():
                    print(f"  {comp:18s}: {value:8.4f}")
                print()

if __name__ == "__main__":
    demonstrate_reward_systems()
